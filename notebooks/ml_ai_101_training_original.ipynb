{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning & AI 101: Complete Beginner's Guide\n",
    "\n",
    "Welcome to your comprehensive introduction to Machine Learning and Artificial Intelligence! This notebook will take you from zero to hero with hands-on examples and practical exercises.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Data Fundamentals** - Working with data using NumPy and Pandas\n",
    "2. **Data Visualization** - Creating meaningful plots and charts\n",
    "3. **Supervised Learning** - Classification and Regression\n",
    "4. **Unsupervised Learning** - Clustering and Dimensionality Reduction\n",
    "5. **Model Evaluation** - How to measure and improve model performance\n",
    "6. **Deep Learning Basics** - Introduction to Neural Networks\n",
    "7. **Real-world Project** - End-to-end ML pipeline\n",
    "\n",
    "## Prerequisites\n",
    "- Basic Python knowledge\n",
    "- Curiosity and willingness to experiment!\n",
    "\n",
    "Let's start your ML journey! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Check\n",
    "\n",
    "First, let's import all the libraries we'll need and check our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import load_iris, make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Fundamentals with NumPy and Pandas\n",
    "\n",
    "Understanding data is the foundation of ML. Let's start with the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 NumPy Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating arrays\n",
    "arr1d = np.array([1, 2, 3, 4, 5])\n",
    "arr2d = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"1D Array:\", arr1d)\n",
    "print(\"2D Array:\\n\", arr2d)\n",
    "print(\"Shape of 2D array:\", arr2d.shape)\n",
    "\n",
    "# Common operations\n",
    "print(\"\\n--- Array Operations ---\")\n",
    "print(\"Mean:\", np.mean(arr1d))\n",
    "print(\"Standard deviation:\", np.std(arr1d))\n",
    "print(\"Element-wise square:\", arr1d ** 2)\n",
    "\n",
    "# Random data generation (crucial for ML)\n",
    "np.random.seed(42)  # For reproducible results\n",
    "random_data = np.random.normal(0, 1, 100)  # 100 random numbers from normal distribution\n",
    "print(f\"\\nGenerated {len(random_data)} random numbers\")\n",
    "print(f\"Mean: {np.mean(random_data):.3f}, Std: {np.std(random_data):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pandas for Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sample dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'age': np.random.randint(18, 65, 100),\n",
    "    'income': np.random.normal(50000, 15000, 100),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 100),\n",
    "    'satisfaction': np.random.randint(1, 11, 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n--- Data Summary ---\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n--- Data Types ---\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n--- Missing Values ---\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add some missing values to demonstrate cleaning\n",
    "df_with_missing = df.copy()\n",
    "df_with_missing.loc[0:4, 'income'] = np.nan\n",
    "df_with_missing.loc[10:14, 'age'] = np.nan\n",
    "\n",
    "print(\"Missing values after introduction:\")\n",
    "print(df_with_missing.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "# Option 1: Drop rows with missing values\n",
    "df_dropped = df_with_missing.dropna()\n",
    "print(f\"\\nAfter dropping missing values: {df_dropped.shape[0]} rows\")\n",
    "\n",
    "# Option 2: Fill missing values\n",
    "df_filled = df_with_missing.copy()\n",
    "df_filled['income'].fillna(df_filled['income'].mean(), inplace=True)\n",
    "df_filled['age'].fillna(df_filled['age'].median(), inplace=True)\n",
    "\n",
    "print(f\"After filling missing values: {df_filled.isnull().sum().sum()} missing values\")\n",
    "\n",
    "# Encoding categorical variables\n",
    "le = LabelEncoder()\n",
    "df_encoded = df_filled.copy()\n",
    "df_encoded['education_encoded'] = le.fit_transform(df_encoded['education'])\n",
    "\n",
    "print(\"\\nEducation encoding:\")\n",
    "print(pd.DataFrame({'Original': df_encoded['education'].unique(), \n",
    "                   'Encoded': le.transform(df_encoded['education'].unique())}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization - Making Data Speak\n",
    "\n",
    "Visualization is crucial for understanding your data before applying ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real dataset for visualization\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target_names[iris.target]\n",
    "\n",
    "print(\"Iris dataset shape:\", iris_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization dashboard\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Iris Dataset Exploration', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Histogram of sepal length\n",
    "axes[0, 0].hist(iris_df['sepal length (cm)'], bins=20, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Sepal Length')\n",
    "axes[0, 0].set_xlabel('Sepal Length (cm)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Box plot by species\n",
    "sns.boxplot(data=iris_df, x='species', y='petal length (cm)', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Petal Length by Species')\n",
    "\n",
    "# 3. Scatter plot\n",
    "for species in iris_df['species'].unique():\n",
    "    species_data = iris_df[iris_df['species'] == species]\n",
    "    axes[0, 2].scatter(species_data['sepal length (cm)'], species_data['sepal width (cm)'], \n",
    "                      label=species, alpha=0.7)\n",
    "axes[0, 2].set_title('Sepal Length vs Width')\n",
    "axes[0, 2].set_xlabel('Sepal Length (cm)')\n",
    "axes[0, 2].set_ylabel('Sepal Width (cm)')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# 4. Correlation heatmap\n",
    "correlation_matrix = iris_df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Feature Correlation Heatmap')\n",
    "\n",
    "# 5. Pairplot (simplified)\n",
    "sns.scatterplot(data=iris_df, x='petal length (cm)', y='petal width (cm)', \n",
    "                hue='species', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Petal Length vs Width by Species')\n",
    "\n",
    "# 6. Distribution comparison\n",
    "for species in iris_df['species'].unique():\n",
    "    species_data = iris_df[iris_df['species'] == species]['sepal length (cm)']\n",
    "    axes[1, 2].hist(species_data, alpha=0.5, label=species, bins=15)\n",
    "axes[1, 2].set_title('Sepal Length Distribution by Species')\n",
    "axes[1, 2].set_xlabel('Sepal Length (cm)')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🎨 Visualization complete! What patterns do you notice?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised Learning - Learning from Examples\n",
    "\n",
    "Supervised learning uses labeled data to make predictions. Let's explore both classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Classification - Predicting Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the iris dataset for classification\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"Classes:\", iris.target_names)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different classification algorithms\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Support Vector Machine': SVC(kernel='rbf', random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "algorithms = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "\n",
    "bars = plt.bar(algorithms, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "plt.title('Classification Algorithm Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Regression - Predicting Continuous Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic regression dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X_reg = np.random.randn(n_samples, 1)\n",
    "y_reg = 2 * X_reg.ravel() + 1 + 0.5 * np.random.randn(n_samples)\n",
    "\n",
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train different regression models\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (name, regressor) in enumerate(regressors.items()):\n",
    "    # Train the model\n",
    "    regressor.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_reg = regressor.predict(X_test_reg)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "    r2 = regressor.score(X_test_reg, y_test_reg)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Squared Error: {mse:.3f}\")\n",
    "    print(f\"  R² Score: {r2:.3f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.scatter(X_test_reg, y_test_reg, alpha=0.6, label='True values')\n",
    "    \n",
    "    # Sort for smooth line plotting\n",
    "    sort_idx = np.argsort(X_test_reg.ravel())\n",
    "    plt.plot(X_test_reg[sort_idx], y_pred_reg[sort_idx], 'r-', linewidth=2, label='Predictions')\n",
    "    \n",
    "    plt.title(f'{name}\\nMSE: {mse:.3f}, R²: {r2:.3f}')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Unsupervised Learning - Finding Hidden Patterns\n",
    "\n",
    "Unsupervised learning finds patterns in data without labeled examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Clustering - Grouping Similar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for clustering\n",
    "X_cluster, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Visualize clustering results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_cluster[:, 0], X_cluster[:, 1], alpha=0.6)\n",
    "plt.title('Original Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Clustered data\n",
    "plt.subplot(1, 3, 2)\n",
    "colors = ['red', 'blue', 'green', 'purple']\n",
    "for i in range(4):\n",
    "    cluster_points = X_cluster[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "               c=colors[i], label=f'Cluster {i}', alpha=0.6)\n",
    "\n",
    "# Plot cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=200, linewidths=3, label='Centers')\n",
    "plt.title('K-Means Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "\n",
    "# Elbow method to find optimal number of clusters\n",
    "plt.subplot(1, 3, 3)\n",
    "k_range = range(1, 11)\n",
    "inertias = []\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_cluster)\n",
    "    inertias.append(kmeans_temp.inertia_)\n",
    "\n",
    "plt.plot(k_range, inertias, 'bo-')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ K-Means completed with {len(set(cluster_labels))} clusters\")\n",
    "print(f\"Inertia (sum of squared distances to centroids): {kmeans.inertia_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dimensionality Reduction - Simplifying Complex Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to the iris dataset\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(iris.data)\n",
    "\n",
    "print(\"Original data shape:\", iris.data.shape)\n",
    "print(\"Reduced data shape:\", X_pca.shape)\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "# Visualize PCA results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original data (first 2 features)\n",
    "plt.subplot(1, 3, 1)\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = iris.target == i\n",
    "    plt.scatter(iris.data[mask, 0], iris.data[mask, 1], \n",
    "               label=species, alpha=0.7)\n",
    "plt.title('Original Data (First 2 Features)')\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.legend()\n",
    "\n",
    "# PCA transformed data\n",
    "plt.subplot(1, 3, 2)\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = iris.target == i\n",
    "    plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "               label=species, alpha=0.7)\n",
    "plt.title('PCA Transformed Data')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.legend()\n",
    "\n",
    "# Explained variance\n",
    "plt.subplot(1, 3, 3)\n",
    "pca_full = PCA()\n",
    "pca_full.fit(iris.data)\n",
    "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(range(1, 5), pca_full.explained_variance_ratio_, 'bo-', label='Individual')\n",
    "plt.plot(range(1, 5), cumsum_variance, 'ro-', label='Cumulative')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Validation\n",
    "\n",
    "Understanding how well your model performs is crucial for ML success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for robust model evaluation\n",
    "from sklearn.model_selection import cross_val_score, validation_curve\n",
    "\n",
    "# Prepare data\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Test different models with cross-validation\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # 5-fold cross-validation\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean CV Accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "    print(f\"  Individual scores: {scores}\")\n",
    "    print()\n",
    "\n",
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Box plot of CV scores\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot(cv_results.values(), labels=cv_results.keys())\n",
    "plt.title('Cross-Validation Score Distribution')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Validation curve for Random Forest\n",
    "plt.subplot(1, 2, 2)\n",
    "param_range = [1, 5, 10, 20, 50, 100]\n",
    "train_scores, val_scores = validation_curve(\n",
    "    RandomForestClassifier(random_state=42), X, y,\n",
    "    param_name='n_estimators', param_range=param_range,\n",
    "    cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean, 'o-', color='blue', label='Training score')\n",
    "plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "plt.plot(param_range, val_mean, 'o-', color='red', label='Validation score')\n",
    "plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.title('Validation Curve (Random Forest)')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed confusion matrix analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names, \n",
    "            yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': iris.feature_names,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "for i, (feature, importance) in enumerate(zip(feature_importance['feature'], feature_importance['importance'])):\n",
    "    print(f\"{i+1}. {feature}: {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deep Learning Basics - Neural Networks\n",
    "\n",
    "Let's explore the fundamentals of neural networks using a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network implementation using sklearn's MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Prepare data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(iris.data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train neural networks with different architectures\n",
    "nn_architectures = {\n",
    "    'Small NN (5 neurons)': MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000, random_state=42),\n",
    "    'Medium NN (10, 5)': MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, random_state=42),\n",
    "    'Large NN (20, 10, 5)': MLPClassifier(hidden_layer_sizes=(20, 10, 5), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "nn_results = {}\n",
    "\n",
    "for name, nn in nn_architectures.items():\n",
    "    # Train the neural network\n",
    "    nn.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = nn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    nn_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'iterations': nn.n_iter_,\n",
    "        'loss': nn.loss_\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  Training iterations: {nn.n_iter_}\")\n",
    "    print(f\"  Final loss: {nn.loss_:.6f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize neural network performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "architectures = list(nn_results.keys())\n",
    "accuracies = [nn_results[arch]['accuracy'] for arch in architectures]\n",
    "plt.bar(architectures, accuracies, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Neural Network Architecture Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.3f}', ha='center')\n",
    "\n",
    "# Loss comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "losses = [nn_results[arch]['loss'] for arch in architectures]\n",
    "plt.bar(architectures, losses, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Final Training Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Neural Network Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of different activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Plot activation functions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x, sigmoid(x), 'b-', linewidth=2)\n",
    "plt.title('Sigmoid Activation')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x, tanh(x), 'r-', linewidth=2)\n",
    "plt.title('Tanh Activation')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(x, relu(x), 'g-', linewidth=2)\n",
    "plt.title('ReLU Activation')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🧠 Key Neural Network Concepts:\")\n",
    "print(\"1. Sigmoid: Smooth curve, outputs between 0-1, can suffer from vanishing gradients\")\n",
    "print(\"2. Tanh: Similar to sigmoid but outputs between -1 and 1\")\n",
    "print(\"3. ReLU: Simple and effective, helps with vanishing gradient problem\")\n",
    "print(\"\\n💡 Modern deep learning mostly uses ReLU and its variants!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-World Project: Complete ML Pipeline\n",
    "\n",
    "Let's build a complete machine learning pipeline from start to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex synthetic dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic customer data for churn prediction\n",
    "n_customers = 1000\n",
    "\n",
    "data = {\n",
    "    'age': np.random.randint(18, 80, n_customers),\n",
    "    'monthly_charges': np.random.normal(70, 20, n_customers),\n",
    "    'total_charges': np.random.normal(2000, 1000, n_customers),\n",
    "    'contract_length': np.random.choice([1, 12, 24], n_customers, p=[0.3, 0.4, 0.3]),\n",
    "    'num_services': np.random.randint(1, 8, n_customers),\n",
    "    'support_calls': np.random.poisson(2, n_customers),\n",
    "    'satisfaction_score': np.random.randint(1, 11, n_customers)\n",
    "}\n",
    "\n",
    "# Create target variable (churn) with logical relationships\n",
    "churn_probability = (\n",
    "    0.1 +  # Base probability\n",
    "    0.2 * (data['satisfaction_score'] <= 5) +  # Low satisfaction increases churn\n",
    "    0.15 * (data['support_calls'] >= 5) +  # Many support calls increase churn\n",
    "    0.1 * (data['monthly_charges'] >= 90) +  # High charges increase churn\n",
    "    -0.1 * (data['contract_length'] == 24) +  # Long contracts reduce churn\n",
    "    0.05 * np.random.random(n_customers)  # Random noise\n",
    ")\n",
    "\n",
    "data['churn'] = np.random.binomial(1, np.clip(churn_probability, 0, 1), n_customers)\n",
    "\n",
    "# Create DataFrame\n",
    "df_project = pd.DataFrame(data)\n",
    "\n",
    "print(\"🎯 Customer Churn Prediction Dataset\")\n",
    "print(f\"Dataset shape: {df_project.shape}\")\n",
    "print(f\"Churn rate: {df_project['churn'].mean():.1%}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df_project.head())\n",
    "\n",
    "print(\"\\nDataset summary:\")\n",
    "print(df_project.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive EDA\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Customer Churn Analysis - Exploratory Data Analysis', fontsize=16)\n",
    "\n",
    "# 1. Churn distribution\n",
    "churn_counts = df_project['churn'].value_counts()\n",
    "axes[0, 0].pie(churn_counts.values, labels=['No Churn', 'Churn'], autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('Churn Distribution')\n",
    "\n",
    "# 2. Age vs Churn\n",
    "for churn_status in [0, 1]:\n",
    "    ages = df_project[df_project['churn'] == churn_status]['age']\n",
    "    axes[0, 1].hist(ages, alpha=0.7, label=f'Churn: {churn_status}', bins=20)\n",
    "axes[0, 1].set_title('Age Distribution by Churn')\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Monthly charges vs Churn\n",
    "sns.boxplot(data=df_project, x='churn', y='monthly_charges', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Monthly Charges by Churn Status')\n",
    "\n",
    "# 4. Support calls vs Churn\n",
    "support_churn = df_project.groupby(['support_calls', 'churn']).size().unstack(fill_value=0)\n",
    "support_churn_rate = support_churn[1] / (support_churn[0] + support_churn[1])\n",
    "axes[1, 0].plot(support_churn_rate.index, support_churn_rate.values, 'o-')\n",
    "axes[1, 0].set_title('Churn Rate by Support Calls')\n",
    "axes[1, 0].set_xlabel('Number of Support Calls')\n",
    "axes[1, 0].set_ylabel('Churn Rate')\n",
    "\n",
    "# 5. Satisfaction score vs Churn\n",
    "satisfaction_churn = df_project.groupby(['satisfaction_score', 'churn']).size().unstack(fill_value=0)\n",
    "satisfaction_churn_rate = satisfaction_churn[1] / (satisfaction_churn[0] + satisfaction_churn[1])\n",
    "axes[1, 1].plot(satisfaction_churn_rate.index, satisfaction_churn_rate.values, 'o-', color='red')\n",
    "axes[1, 1].set_title('Churn Rate by Satisfaction Score')\n",
    "axes[1, 1].set_xlabel('Satisfaction Score')\n",
    "axes[1, 1].set_ylabel('Churn Rate')\n",
    "\n",
    "# 6. Correlation heatmap\n",
    "correlation = df_project.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df_processed = df_project.copy()\n",
    "\n",
    "# Create new features\n",
    "df_processed['avg_monthly_charges'] = df_processed['total_charges'] / (df_processed['contract_length'] * 12)\n",
    "df_processed['charges_per_service'] = df_processed['monthly_charges'] / df_processed['num_services']\n",
    "df_processed['high_satisfaction'] = (df_processed['satisfaction_score'] >= 8).astype(int)\n",
    "df_processed['high_support_calls'] = (df_processed['support_calls'] >= 3).astype(int)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = ['age', 'monthly_charges', 'total_charges', 'contract_length', \n",
    "                  'num_services', 'support_calls', 'satisfaction_score',\n",
    "                  'avg_monthly_charges', 'charges_per_service', 'high_satisfaction', 'high_support_calls']\n",
    "\n",
    "X = df_processed[feature_columns]\n",
    "y = df_processed['churn']\n",
    "\n",
    "print(\"🔧 Feature Engineering Complete\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(\"Features:\", list(X.columns))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training churn rate: {y_train.mean():.1%}\")\n",
    "print(f\"Testing churn rate: {y_test.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for SVM and Neural Network\n",
    "    if name in ['SVM', 'Neural Network']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    if name in ['SVM', 'Neural Network']:\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    print()\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "model_names = list(model_results.keys())\n",
    "accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "cv_means = [model_results[name]['cv_mean'] for name in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, accuracies, width, label='Test Accuracy', alpha=0.8)\n",
    "plt.bar(x + width/2, cv_means, width, label='CV Mean', alpha=0.8)\n",
    "\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(x, model_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance (Random Forest)\n",
    "plt.subplot(1, 3, 2)\n",
    "rf_model = models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "plt.title('Feature Importance (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "\n",
    "# ROC Curve comparison\n",
    "plt.subplot(1, 3, 3)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "for name in model_names:\n",
    "    fpr, tpr, _ = roc_curve(y_test, model_results[name]['probabilities'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.title('ROC Curves')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model Interpretation and Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model (Random Forest for interpretability)\n",
    "best_model = models['Random Forest']\n",
    "best_predictions = model_results['Random Forest']['predictions']\n",
    "\n",
    "# Detailed analysis\n",
    "print(\"🎯 CUSTOMER CHURN PREDICTION - BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📊 Model Performance (Random Forest):\")\n",
    "print(f\"   • Accuracy: {model_results['Random Forest']['accuracy']:.1%}\")\n",
    "print(f\"   • Cross-validation: {model_results['Random Forest']['cv_mean']:.1%} (+/- {model_results['Random Forest']['cv_std'] * 2:.1%})\")\n",
    "\n",
    "# Feature importance insights\n",
    "print(f\"\\n🔍 Top 5 Churn Predictors:\")\n",
    "top_features = feature_importance.tail(5)\n",
    "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "    print(f\"   {i}. {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# Business recommendations\n",
    "print(f\"\\n💼 Business Recommendations:\")\n",
    "print(f\"   • Focus on customer satisfaction (score < 8 indicates high churn risk)\")\n",
    "print(f\"   • Reduce support calls through better service quality\")\n",
    "print(f\"   • Consider pricing strategies for high monthly charges\")\n",
    "print(f\"   • Promote longer contract lengths to reduce churn\")\n",
    "\n",
    "# Confusion matrix for final model\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Churn', 'Churn'], \n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "plt.title('Confusion Matrix - Random Forest Model')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate business metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\n📈 Detailed Metrics:\")\n",
    "print(f\"   • Precision: {precision:.3f} (of predicted churners, {precision:.1%} actually churned)\")\n",
    "print(f\"   • Recall: {recall:.3f} (caught {recall:.1%} of actual churners)\")\n",
    "print(f\"   • F1-Score: {f1:.3f}\")\n",
    "print(f\"\\n   • True Positives: {tp} (correctly identified churners)\")\n",
    "print(f\"   • False Positives: {fp} (incorrectly flagged as churners)\")\n",
    "print(f\"   • False Negatives: {fn} (missed churners)\")\n",
    "print(f\"   • True Negatives: {tn} (correctly identified non-churners)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practice Exercises and Next Steps\n",
    "\n",
    "Now it's your turn to practice! Here are some exercises to deepen your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 1: Modify the Churn Prediction Model\n",
    "\n",
    "Try these modifications to improve the model:\n",
    "1. Add more engineered features\n",
    "2. Try different hyperparameters\n",
    "3. Handle class imbalance if present\n",
    "4. Use ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Try creating new features or tuning hyperparameters\n",
    "\n",
    "print(\"💡 Exercise 1: Try these ideas:\")\n",
    "print(\"1. Create interaction features (e.g., age * satisfaction_score)\")\n",
    "print(\"2. Use GridSearchCV to find optimal hyperparameters\")\n",
    "print(\"3. Try ensemble methods like Voting or Stacking\")\n",
    "print(\"4. Handle class imbalance with SMOTE or class weights\")\n",
    "\n",
    "# Example: Create a new feature\n",
    "# df_processed['age_satisfaction_interaction'] = df_processed['age'] * df_processed['satisfaction_score']\n",
    "\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 2: Regression Challenge\n",
    "\n",
    "Create a regression model to predict customer lifetime value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Regression practice\n",
    "print(\"💡 Exercise 2: Build a regression model\")\n",
    "print(\"Goal: Predict customer lifetime value using available features\")\n",
    "print(\"\\nSteps:\")\n",
    "print(\"1. Create a synthetic 'lifetime_value' target variable\")\n",
    "print(\"2. Use regression algorithms (Linear, Random Forest, etc.)\")\n",
    "print(\"3. Evaluate using MSE, MAE, and R²\")\n",
    "print(\"4. Visualize predictions vs actual values\")\n",
    "\n",
    "# Hint: Create lifetime value based on logical relationships\n",
    "# lifetime_value = base_value + (monthly_charges * contract_length * loyalty_factor) + noise\n",
    "\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Exercise 3: Clustering Analysis\n",
    "\n",
    "Segment customers using unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Customer segmentation\n",
    "print(\"💡 Exercise 3: Customer Segmentation\")\n",
    "print(\"Goal: Group customers into meaningful segments\")\n",
    "print(\"\\nSteps:\")\n",
    "print(\"1. Use K-Means clustering on customer features\")\n",
    "print(\"2. Determine optimal number of clusters using elbow method\")\n",
    "print(\"3. Analyze cluster characteristics\")\n",
    "print(\"4. Visualize clusters using PCA\")\n",
    "print(\"5. Create business-meaningful cluster names\")\n",
    "\n",
    "# Hint: Focus on behavioral features like charges, services, satisfaction\n",
    "\n",
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps in Your ML Journey 🚀\n",
    "\n",
    "Congratulations! You've completed the ML & AI 101 training. Here's your roadmap for continued learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📚 Immediate Next Steps (1-2 weeks)\n",
    "1. **Practice with Real Datasets**\n",
    "   - Kaggle competitions and datasets\n",
    "   - UCI Machine Learning Repository\n",
    "   - Government open data portals\n",
    "\n",
    "2. **Master Key Libraries**\n",
    "   - Advanced Pandas operations\n",
    "   - Scikit-learn pipelines\n",
    "   - Data visualization with Plotly\n",
    "\n",
    "3. **Learn Model Evaluation**\n",
    "   - Cross-validation strategies\n",
    "   - Hyperparameter tuning\n",
    "   - Model interpretation techniques\n",
    "\n",
    "### 🎯 Intermediate Goals (1-3 months)\n",
    "1. **Deep Learning**\n",
    "   - TensorFlow or PyTorch fundamentals\n",
    "   - Convolutional Neural Networks (CNNs)\n",
    "   - Recurrent Neural Networks (RNNs)\n",
    "\n",
    "2. **Specialized Areas**\n",
    "   - Natural Language Processing (NLP)\n",
    "   - Computer Vision\n",
    "   - Time Series Analysis\n",
    "\n",
    "3. **MLOps & Production**\n",
    "   - Model deployment\n",
    "   - Version control for ML\n",
    "   - Monitoring and maintenance\n",
    "\n",
    "### 🏆 Advanced Goals (3-6 months)\n",
    "1. **Research & Innovation**\n",
    "   - Read ML research papers\n",
    "   - Implement state-of-the-art models\n",
    "   - Contribute to open-source projects\n",
    "\n",
    "2. **Business Application**\n",
    "   - End-to-end ML projects\n",
    "   - Business metrics and ROI\n",
    "   - Stakeholder communication\n",
    "\n",
    "### 💡 Learning Resources\n",
    "- **Books**: \"Hands-On ML\" by Aurélien Géron, \"Pattern Recognition\" by Bishop\n",
    "- **Courses**: Coursera ML Course, Fast.ai, Udacity ML Nanodegree\n",
    "- **Practice**: Kaggle, Google Colab, Personal projects\n",
    "- **Community**: Reddit r/MachineLearning, ML Twitter, Local meetups\n",
    "\n",
    "### ✅ Final Checklist\n",
    "- [ ] Complete all exercises in this notebook\n",
    "- [ ] Try a Kaggle competition\n",
    "- [ ] Build your first end-to-end ML project\n",
    "- [ ] Share your work on GitHub\n",
    "- [ ] Join ML communities and start networking\n",
    "\n",
    "**Remember**: Machine Learning is a journey, not a destination. Keep practicing, stay curious, and don't be afraid to experiment! 🌟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📝 Summary\n",
    "\n",
    "In this comprehensive ML & AI 101 notebook, you've learned:\n",
    "\n",
    "✅ **Data Fundamentals** - NumPy arrays, Pandas DataFrames, data cleaning\n",
    "\n",
    "✅ **Visualization** - Creating meaningful plots to understand your data\n",
    "\n",
    "✅ **Supervised Learning** - Classification and regression with multiple algorithms\n",
    "\n",
    "✅ **Unsupervised Learning** - Clustering and dimensionality reduction\n",
    "\n",
    "✅ **Model Evaluation** - Cross-validation, metrics, and performance assessment\n",
    "\n",
    "✅ **Deep Learning Basics** - Neural networks and activation functions\n",
    "\n",
    "✅ **Complete ML Pipeline** - End-to-end project from data to insights\n",
    "\n",
    "You're now equipped with the foundational knowledge to tackle real-world ML problems. Keep practicing, stay curious, and happy learning! 🎉\n",
    "\n",
    "---\n",
    "*Created for ML & AI beginners • Feel free to modify and extend this notebook for your learning needs*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
