{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world data integration strategies\\n\\nimport sqlite3\\nimport json\\nfrom urllib.parse import urljoin\\nfrom datetime import datetime, timedelta\\nimport time\\n\\nclass DataConnector:\\n    \\\"\\\"\\\"Professional data connector for various sources\\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.connection_cache = {}\\n        self.request_session = None\\n        if HAS_REQUESTS:\\n            self.request_session = requests.Session()\\n            self.request_session.headers.update({\\n                'User-Agent': 'ML-Training-Notebook/1.0'\\n            })\\n    \\n    def connect_to_api(self, base_url, endpoint, params=None, headers=None, rate_limit=1.0):\\n        \\\"\\\"\\\"Connect to REST API with rate limiting and error handling\\\"\\\"\\\"\\n        \\n        if not HAS_REQUESTS:\\n            print(\\\"‚ùå Requests library not available. Install with: pip install requests\\\")\\n            return None\\n        \\n        try:\\n            url = urljoin(base_url, endpoint)\\n            \\n            # Rate limiting\\n            time.sleep(rate_limit)\\n            \\n            # Add custom headers\\n            session_headers = self.request_session.headers.copy()\\n            if headers:\\n                session_headers.update(headers)\\n            \\n            print(f\\\"üåê Connecting to API: {url}\\\")\\n            response = self.request_session.get(url, params=params, headers=session_headers, timeout=10)\\n            response.raise_for_status()\\n            \\n            print(f\\\"‚úÖ API request successful (Status: {response.status_code})\\\")\\n            return response.json()\\n            \\n        except requests.exceptions.RequestException as e:\\n            print(f\\\"‚ùå API request failed: {e}\\\")\\n            return None\\n        except json.JSONDecodeError as e:\\n            print(f\\\"‚ùå Failed to parse JSON response: {e}\\\")\\n            return None\\n    \\n    def create_sample_database(self, db_path='sample_data.db'):\\n        \\\"\\\"\\\"Create a sample SQLite database for demonstration\\\"\\\"\\\"\\n        \\n        print(f\\\"üóÑÔ∏è Creating sample database: {db_path}\\\")\\n        \\n        # Create connection\\n        conn = sqlite3.connect(db_path)\\n        cursor = conn.cursor()\\n        \\n        # Create tables\\n        cursor.execute('''\\n            CREATE TABLE IF NOT EXISTS customers (\\n                customer_id INTEGER PRIMARY KEY,\\n                name TEXT NOT NULL,\\n                email TEXT UNIQUE,\\n                registration_date DATE,\\n                country TEXT,\\n                subscription_tier TEXT\\n            )\\n        ''')\\n        \\n        cursor.execute('''\\n            CREATE TABLE IF NOT EXISTS transactions (\\n                transaction_id INTEGER PRIMARY KEY,\\n                customer_id INTEGER,\\n                amount REAL,\\n                transaction_date DATETIME,\\n                product_category TEXT,\\n                FOREIGN KEY (customer_id) REFERENCES customers (customer_id)\\n            )\\n        ''')\\n        \\n        # Insert sample data\\n        np.random.seed(RANDOM_STATE)\\n        \\n        # Generate customers\\n        countries = ['USA', 'UK', 'Germany', 'France', 'Canada', 'Australia']\\n        tiers = ['Basic', 'Premium', 'Enterprise']\\n        \\n        customers_data = []\\n        for i in range(1, 201):  # 200 customers\\n            customers_data.append((\\n                i,\\n                f\\\"Customer_{i:03d}\\\",\\n                f\\\"customer{i:03d}@email.com\\\",\\n                (datetime.now() - timedelta(days=np.random.randint(30, 365))).date(),\\n                np.random.choice(countries),\\n                np.random.choice(tiers, p=[0.5, 0.35, 0.15])\\n            ))\\n        \\n        cursor.executemany(\\n            'INSERT OR REPLACE INTO customers VALUES (?, ?, ?, ?, ?, ?)',\\n            customers_data\\n        )\\n        \\n        # Generate transactions\\n        categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports']\\n        transactions_data = []\\n        \\n        for i in range(1, 1001):  # 1000 transactions\\n            customer_id = np.random.randint(1, 201)\\n            amount = np.random.lognormal(3, 1)  # Log-normal distribution for realistic amounts\\n            transaction_date = datetime.now() - timedelta(days=np.random.randint(0, 90))\\n            category = np.random.choice(categories)\\n            \\n            transactions_data.append((\\n                i, customer_id, round(amount, 2), transaction_date, category\\n            ))\\n        \\n        cursor.executemany(\\n            'INSERT OR REPLACE INTO transactions VALUES (?, ?, ?, ?, ?)',\\n            transactions_data\\n        )\\n        \\n        conn.commit()\\n        conn.close()\\n        \\n        print(f\\\"‚úÖ Database created with {len(customers_data)} customers and {len(transactions_data)} transactions\\\")\\n        return db_path\\n    \\n    def connect_to_database(self, db_path, query):\\n        \\\"\\\"\\\"Connect to SQLite database and execute query\\\"\\\"\\\"\\n        \\n        try:\\n            print(f\\\"üóÑÔ∏è Connecting to database: {db_path}\\\")\\n            conn = sqlite3.connect(db_path)\\n            \\n            # Execute query and return DataFrame\\n            df = pd.read_sql_query(query, conn)\\n            conn.close()\\n            \\n            print(f\\\"‚úÖ Query executed successfully. Retrieved {len(df)} rows\\\")\\n            return df\\n            \\n        except sqlite3.Error as e:\\n            print(f\\\"‚ùå Database error: {e}\\\")\\n            return None\\n    \\n    def simulate_web_scraping(self, num_records=100):\\n        \\\"\\\"\\\"Simulate web scraping (without actual scraping)\\\"\\\"\\\"\\n        \\n        print(f\\\"üï∑Ô∏è Simulating web scraping for {num_records} records...\\\")\\n        \\n        # Simulate realistic web-scraped data\\n        np.random.seed(RANDOM_STATE)\\n        \\n        # Simulate product data from e-commerce site\\n        products = []\\n        categories = ['Electronics', 'Books', 'Clothing', 'Home & Garden', 'Sports']\\n        brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']\\n        \\n        for i in range(num_records):\\n            # Simulate some missing data (realistic for web scraping)\\n            rating = np.random.uniform(1, 5) if np.random.random() > 0.1 else None\\n            price = np.random.lognormal(3, 0.8) if np.random.random() > 0.05 else None\\n            \\n            product = {\\n                'product_id': f'P{i:04d}',\\n                'name': f'Product {i}',\\n                'category': np.random.choice(categories),\\n                'brand': np.random.choice(brands) if np.random.random() > 0.15 else None,\\n                'price': round(price, 2) if price else None,\\n                'rating': round(rating, 1) if rating else None,\\n                'num_reviews': np.random.poisson(50) if rating else 0,\\n                'in_stock': np.random.choice([True, False], p=[0.85, 0.15]),\\n                'scraped_date': datetime.now() - timedelta(hours=np.random.randint(0, 24))\\n            }\\n            products.append(product)\\n        \\n        df = pd.DataFrame(products)\\n        \\n        print(f\\\"‚úÖ Simulated scraping complete. Created dataset with shape {df.shape}\\\")\\n        print(f\\\"Missing data: {df.isnull().sum().sum()} total missing values\\\")\\n        \\n        return df\\n    \\n    def fetch_public_api_data(self, api_name='jsonplaceholder'):\\n        \\\"\\\"\\\"Fetch data from public APIs for demonstration\\\"\\\"\\\"\\n        \\n        if not HAS_REQUESTS:\\n            print(\\\"‚ùå Requests library not available\\\")\\n            return None\\n        \\n        if api_name == 'jsonplaceholder':\\n            # JSONPlaceholder - fake REST API\\n            print(\\\"üì° Fetching data from JSONPlaceholder API...\\\")\\n            \\n            # Fetch users\\n            users_data = self.connect_to_api(\\n                'https://jsonplaceholder.typicode.com/',\\n                'users'\\n            )\\n            \\n            # Fetch posts\\n            posts_data = self.connect_to_api(\\n                'https://jsonplaceholder.typicode.com/',\\n                'posts'\\n            )\\n            \\n            if users_data and posts_data:\\n                users_df = pd.DataFrame(users_data)\\n                posts_df = pd.DataFrame(posts_data)\\n                \\n                # Clean and normalize the data\\n                users_clean = pd.json_normalize(users_data)\\n                posts_clean = pd.DataFrame(posts_data)\\n                \\n                print(f\\\"‚úÖ Fetched {len(users_clean)} users and {len(posts_clean)} posts\\\")\\n                return {'users': users_clean, 'posts': posts_clean}\\n        \\n        return None\\n\\n# Initialize data connector\\ndata_connector = DataConnector()\\n\\n# Demonstrate different data sources\\nprint(\\\"üåç REAL-WORLD DATA INTEGRATION EXAMPLES\\\")\\nprint(\\\"=\\\" * 50)\\n\\n# 1. Database connection\\ndb_path = data_connector.create_sample_database()\\n\\n# Query customer data\\ncustomer_query = '''\\n    SELECT \\n        c.customer_id,\\n        c.name,\\n        c.country,\\n        c.subscription_tier,\\n        COUNT(t.transaction_id) as num_transactions,\\n        SUM(t.amount) as total_spent,\\n        AVG(t.amount) as avg_transaction\\n    FROM customers c\\n    LEFT JOIN transactions t ON c.customer_id = t.customer_id\\n    GROUP BY c.customer_id\\n    ORDER BY total_spent DESC\\n    LIMIT 10\\n'''\\n\\ncustomer_data = data_connector.connect_to_database(db_path, customer_query)\\nif customer_data is not None:\\n    print(\\\"\\\\nüèÜ Top 10 customers by total spent:\\\")\\n    print(customer_data)\\n\\n# 2. Web scraping simulation\\nscraped_data = data_connector.simulate_web_scraping(50)\\nprint(\\\"\\\\nüï∑Ô∏è Sample scraped data:\\\")\\nprint(scraped_data.head())\\nprint(f\\\"\\\\nData quality check - Missing values per column:\\\")\\nprint(scraped_data.isnull().sum())\\n\\n# 3. Public API data\\napi_data = data_connector.fetch_public_api_data()\\nif api_data:\\n    print(\\\"\\\\nüì° Public API data:\\\")\\n    print(f\\\"Users shape: {api_data['users'].shape}\\\")\\n    print(f\\\"Posts shape: {api_data['posts'].shape}\\\")\\n    print(\\\"\\\\nSample user data:\\\")\\n    print(api_data['users'][['name', 'email', 'address.city', 'company.name']].head())\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Real Data Sources Integration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-World Data Integration & MLOps\\n\\n**‚è±Ô∏è Estimated time:** 50 minutes\\n\\n**Learning objectives:**\\n- Connect to real data sources (APIs, databases, web scraping)\\n- Implement model persistence and versioning\\n- Apply MLOps principles for production deployments\\n- Handle data quality monitoring and model drift detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning & AI 101: Complete Professional Training\n",
    "\n",
    "üéØ **Welcome to the most comprehensive ML/AI training for data professionals!**\n",
    "\n",
    "This enhanced notebook transforms beginners into competent ML practitioners through:\n",
    "- **Systematic skill building** with measurable learning outcomes\n",
    "- **Real-world applications** using actual data sources and deployment techniques\n",
    "- **Industry best practices** including MLOps, testing, and production considerations\n",
    "- **Interactive assessments** to validate your progress\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "\n",
    "By completing this training, you will:\n",
    "\n",
    "1. **Master data preprocessing pipelines** for production-ready ML systems\n",
    "2. **Implement robust model evaluation** with proper validation strategies\n",
    "3. **Build end-to-end ML applications** with real data sources and deployment\n",
    "4. **Apply MLOps principles** for model versioning, monitoring, and maintenance\n",
    "5. **Handle ethical considerations** including bias detection and fairness metrics\n",
    "6. **Debug common ML issues** and optimize model performance\n",
    "\n",
    "**Estimated completion time:** 8-12 hours (can be completed in modules)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Prerequisites & Environment Setup\n",
    "\n",
    "### Required Knowledge\n",
    "- [ ] Basic Python programming (functions, classes, data structures)\n",
    "- [ ] Elementary statistics (mean, variance, distributions)\n",
    "- [ ] High school mathematics (algebra, basic calculus helpful but not required)\n",
    "\n",
    "### Success Criteria\n",
    "- [ ] Complete all checkpoint assessments with 70%+ scores\n",
    "- [ ] Successfully implement at least 2 end-to-end projects\n",
    "- [ ] Demonstrate ability to debug and optimize ML models\n",
    "\n",
    "Let's verify your environment and begin your professional ML journey! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Validation\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 15 minutes\n",
    "\n",
    "**Learning objectives:**\n",
    "- Set up a reproducible ML environment\n",
    "- Understand version management for ML projects\n",
    "- Implement proper random seed management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ML ENVIRONMENT VALIDATION\n",
      "==================================================\n",
      "Python version: 3.12.3\n",
      "NumPy version: 2.3.3\n",
      "Pandas version: 2.3.2\n",
      "Scikit-learn version: 1.7.2\n",
      "Matplotlib version: 3.10.6\n",
      "Seaborn version: 0.13.2\n",
      "\n",
      "üì¶ Optional Libraries:\n",
      "Joblib available: ‚úÖ\n",
      "Requests available: ‚úÖ\n",
      "\n",
      "üé≤ Random state set to: 42\n",
      "üìÖ Session started: 2025-09-28 10:58:12\n",
      "\n",
      "‚úÖ Environment ready for ML training!\n"
     ]
    }
   ],
   "source": [
    "# Environment setup with version tracking and reproducibility\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Core libraries with version checking\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Machine Learning - Core\n",
    "import sklearn\n",
    "from sklearn.datasets import (\n",
    "    load_iris, load_wine, load_breast_cancer, \n",
    "    make_classification, make_regression, make_blobs\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, \n",
    "    RandomizedSearchCV, validation_curve, learning_curve,\n",
    "    StratifiedKFold, KFold\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, \n",
    "    LabelEncoder, OneHotEncoder, PolynomialFeatures\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    GradientBoostingClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    silhouette_score, adjusted_rand_score\n",
    ")\n",
    "\n",
    "# Advanced libraries\n",
    "try:\n",
    "    import joblib\n",
    "    HAS_JOBLIB = True\n",
    "except ImportError:\n",
    "    HAS_JOBLIB = False\n",
    "    \n",
    "try:\n",
    "    import requests\n",
    "    HAS_REQUESTS = True\n",
    "except ImportError:\n",
    "    HAS_REQUESTS = False\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Global random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Environment validation\n",
    "print(\"üîß ML ENVIRONMENT VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "\n",
    "print(\"\\nüì¶ Optional Libraries:\")\n",
    "print(f\"Joblib available: {'‚úÖ' if HAS_JOBLIB else '‚ùå (pip install joblib)'}\")\n",
    "print(f\"Requests available: {'‚úÖ' if HAS_REQUESTS else '‚ùå (pip install requests)'}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random state set to: {RANDOM_STATE}\")\n",
    "print(f\"üìÖ Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n‚úÖ Environment ready for ML training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Checkpoint 1: Environment Validation\n",
    "\n",
    "**Quick Assessment (2 minutes):**\n",
    "\n",
    "1. What is the purpose of setting a random state in ML projects?\n",
    "2. Why do we suppress warnings in production ML code?\n",
    "3. What happens if you don't manage package versions in ML projects?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answers</summary>\n",
    "\n",
    "1. **Random state ensures reproducibility** - same results across runs and different environments\n",
    "2. **Suppress warnings to avoid clutter** in production logs, but keep them during development\n",
    "3. **Version mismatches can cause** model performance changes, crashes, or different results\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Fundamentals & Professional Preprocessing\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 45 minutes\n",
    "\n",
    "**Learning objectives:**\n",
    "- Master production-ready data preprocessing pipelines\n",
    "- Handle missing data with advanced strategies\n",
    "- Implement feature engineering and validation\n",
    "- Understand data leakage and prevention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Advanced Data Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional data validation and quality assessment\n",
    "def validate_dataset(df, name=\"Dataset\"):\n",
    "    \"\"\"Comprehensive data validation function\"\"\"\n",
    "    print(f\"\\nüìä {name} Validation Report\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing': missing,\n",
    "        'Percentage': missing_pct\n",
    "    }).sort_values('Missing', ascending=False)\n",
    "    \n",
    "    if missing.sum() > 0:\n",
    "        print(\"\\n‚ö†Ô∏è Missing Data:\")\n",
    "        print(missing_df[missing_df['Missing'] > 0])\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing data found\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate rows: {duplicates} ({duplicates/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Numeric column statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìà Numeric columns: {len(numeric_cols)}\")\n",
    "        print(\"Range check:\")\n",
    "        for col in numeric_cols:\n",
    "            print(f\"  {col}: [{df[col].min():.3f}, {df[col].max():.3f}]\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Create a comprehensive synthetic dataset for demonstration\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "n_samples = 1000\n",
    "synthetic_data = {\n",
    "    # Demographic features\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'income': np.random.lognormal(10.5, 0.8, n_samples),  # More realistic income distribution\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
    "    \n",
    "    # Behavioral features\n",
    "    'monthly_spend': np.random.gamma(2, 50, n_samples),\n",
    "    'num_purchases': np.random.poisson(8, n_samples),\n",
    "    'days_since_last_purchase': np.random.exponential(10, n_samples),\n",
    "    'customer_rating': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.05, 0.1, 0.2, 0.4, 0.25]),\n",
    "    \n",
    "    # Technical features\n",
    "    'website_visits': np.random.negative_binomial(10, 0.3, n_samples),\n",
    "    'mobile_app_usage': np.random.beta(2, 5, n_samples) * 100,  # Percentage\n",
    "    'email_open_rate': np.random.beta(3, 7, n_samples),\n",
    "}\n",
    "\n",
    "# Add missing values realistically (missing not at random)\n",
    "df_raw = pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Income missing for younger customers (survey bias)\n",
    "young_mask = df_raw['age'] < 25\n",
    "df_raw.loc[young_mask & (np.random.random(sum(young_mask)) < 0.3), 'income'] = np.nan\n",
    "\n",
    "# Rating missing for customers with very few purchases\n",
    "low_purchase_mask = df_raw['num_purchases'] <= 2\n",
    "df_raw.loc[low_purchase_mask & (np.random.random(sum(low_purchase_mask)) < 0.4), 'customer_rating'] = np.nan\n",
    "\n",
    "# App usage missing for older customers\n",
    "old_mask = df_raw['age'] > 65\n",
    "df_raw.loc[old_mask & (np.random.random(sum(old_mask)) < 0.6), 'mobile_app_usage'] = np.nan\n",
    "\n",
    "# Add some extreme outliers\n",
    "outlier_indices = np.random.choice(df_raw.index, 20, replace=False)\n",
    "df_raw.loc[outlier_indices, 'monthly_spend'] *= 10  # Very high spenders\n",
    "\n",
    "# Validate the dataset\n",
    "validation_report = validate_dataset(df_raw, \"Customer Dataset\")\n",
    "\n",
    "print(\"\\nüéØ First 5 rows:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Professional Missing Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced missing data analysis and handling strategies\n",
    "\n",
    "def analyze_missing_patterns(df):\n",
    "    \"\"\"Analyze patterns in missing data\"\"\"\n",
    "    print(\"üîç Missing Data Pattern Analysis\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    missing_df = df.isnull()\n",
    "    \n",
    "    # Missing data patterns\n",
    "    missing_patterns = missing_df.groupby(list(missing_df.columns)).size().reset_index().rename(columns={0:'count'})\n",
    "    missing_patterns = missing_patterns.sort_values('count', ascending=False)\n",
    "    \n",
    "    print(\"Top 5 missing patterns:\")\n",
    "    for i, (_, row) in enumerate(missing_patterns.head().iterrows()):\n",
    "        pattern = [col for col in missing_df.columns if row[col]]\n",
    "        if pattern:\n",
    "            print(f\"  {i+1}. Missing {pattern}: {row['count']} rows\")\n",
    "        else:\n",
    "            print(f\"  {i+1}. Complete data: {row['count']} rows\")\n",
    "    \n",
    "    # Correlation between missing values\n",
    "    missing_corr = missing_df.corr()\n",
    "    \n",
    "    # Visualize missing data patterns\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Missing data heatmap\n",
    "    missing_matrix = df.isnull().astype(int)\n",
    "    sns.heatmap(missing_matrix.corr(), annot=True, cmap='RdYlBu_r', center=0, ax=axes[0])\n",
    "    axes[0].set_title('Missing Data Correlation')\n",
    "    \n",
    "    # Missing data by column\n",
    "    missing_counts = df.isnull().sum().sort_values(ascending=True)\n",
    "    missing_counts = missing_counts[missing_counts > 0]\n",
    "    if len(missing_counts) > 0:\n",
    "        missing_counts.plot(kind='barh', ax=axes[1])\n",
    "        axes[1].set_title('Missing Values by Column')\n",
    "        axes[1].set_xlabel('Number of Missing Values')\n",
    "    \n",
    "    # Missing data pattern visualization\n",
    "    sample_data = df.head(100)  # Sample for visualization\n",
    "    missing_vis = sample_data.isnull().astype(int)\n",
    "    sns.heatmap(missing_vis.T, cbar=True, cmap='RdYlBu_r', ax=axes[2])\n",
    "    axes[2].set_title('Missing Data Pattern (First 100 rows)')\n",
    "    axes[2].set_xlabel('Row Index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return missing_patterns\n",
    "\n",
    "# Multiple imputation strategies\n",
    "def compare_imputation_strategies(df, target_col):\n",
    "    \"\"\"Compare different imputation strategies\"\"\"\n",
    "    print(f\"\\nüîß Imputation Strategy Comparison for '{target_col}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if target_col not in df.columns or df[target_col].isnull().sum() == 0:\n",
    "        print(f\"Column '{target_col}' has no missing values\")\n",
    "        return\n",
    "    \n",
    "    # Original statistics\n",
    "    original_mean = df[target_col].mean()\n",
    "    original_std = df[target_col].std()\n",
    "    missing_count = df[target_col].isnull().sum()\n",
    "    \n",
    "    print(f\"Original - Mean: {original_mean:.3f}, Std: {original_std:.3f}\")\n",
    "    print(f\"Missing values: {missing_count} ({missing_count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    strategies = {\n",
    "        'Mean': SimpleImputer(strategy='mean'),\n",
    "        'Median': SimpleImputer(strategy='median'),\n",
    "        'KNN (k=5)': KNNImputer(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Prepare features for KNN imputation\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df_numeric = df[numeric_cols]\n",
    "    \n",
    "    for name, imputer in strategies.items():\n",
    "        df_imputed = df_numeric.copy()\n",
    "        \n",
    "        if name.startswith('KNN'):\n",
    "            # KNN needs all numeric data\n",
    "            df_imputed = pd.DataFrame(\n",
    "                imputer.fit_transform(df_numeric),\n",
    "                columns=df_numeric.columns,\n",
    "                index=df_numeric.index\n",
    "            )\n",
    "        else:\n",
    "            # Simple imputation\n",
    "            df_imputed[target_col] = imputer.fit_transform(df_numeric[[target_col]]).ravel()\n",
    "        \n",
    "        imputed_mean = df_imputed[target_col].mean()\n",
    "        imputed_std = df_imputed[target_col].std()\n",
    "        \n",
    "        results[name] = {\n",
    "            'mean': imputed_mean,\n",
    "            'std': imputed_std,\n",
    "            'mean_diff': abs(imputed_mean - original_mean),\n",
    "            'std_diff': abs(imputed_std - original_std)\n",
    "        }\n",
    "        \n",
    "        print(f\"{name:12} - Mean: {imputed_mean:.3f}, Std: {imputed_std:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze missing patterns\n",
    "missing_patterns = analyze_missing_patterns(df_raw)\n",
    "\n",
    "# Compare imputation strategies for income\n",
    "imputation_results = compare_imputation_strategies(df_raw, 'income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Production-Ready Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional preprocessing pipeline using sklearn\n",
    "\n",
    "class MLPreprocessor:\n",
    "    \"\"\"Production-ready preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, handle_outliers=True, outlier_method='iqr'):\n",
    "        self.handle_outliers = handle_outliers\n",
    "        self.outlier_method = outlier_method\n",
    "        self.preprocessor = None\n",
    "        self.feature_names = None\n",
    "        self.outlier_bounds = {}\n",
    "        \n",
    "    def detect_outliers(self, X, column, method='iqr'):\n",
    "        \"\"\"Detect outliers using IQR or z-score method\"\"\"\n",
    "        if method == 'iqr':\n",
    "            Q1 = X[column].quantile(0.25)\n",
    "            Q3 = X[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "        else:  # z-score\n",
    "            mean = X[column].mean()\n",
    "            std = X[column].std()\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "        \n",
    "        self.outlier_bounds[column] = (lower_bound, upper_bound)\n",
    "        outliers = (X[column] < lower_bound) | (X[column] > upper_bound)\n",
    "        return outliers\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the preprocessing pipeline\"\"\"\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Separate numeric and categorical columns\n",
    "        numeric_features = X_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X_copy.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Remove ID columns if present\n",
    "        id_columns = [col for col in numeric_features if 'id' in col.lower()]\n",
    "        numeric_features = [col for col in numeric_features if col not in id_columns]\n",
    "        \n",
    "        print(f\"Identified features:\")\n",
    "        print(f\"  Numeric: {len(numeric_features)} - {numeric_features}\")\n",
    "        print(f\"  Categorical: {len(categorical_features)} - {categorical_features}\")\n",
    "        print(f\"  ID columns (excluded): {id_columns}\")\n",
    "        \n",
    "        # Handle outliers in numeric features\n",
    "        if self.handle_outliers:\n",
    "            print(f\"\\nüéØ Outlier Detection ({self.outlier_method} method):\")\n",
    "            for col in numeric_features:\n",
    "                outliers = self.detect_outliers(X_copy, col, self.outlier_method)\n",
    "                outlier_count = outliers.sum()\n",
    "                if outlier_count > 0:\n",
    "                    print(f\"  {col}: {outlier_count} outliers ({outlier_count/len(X_copy)*100:.1f}%)\")\n",
    "        \n",
    "        # Create preprocessing pipelines\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', KNNImputer(n_neighbors=5)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "            ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        # Combine transformers\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ],\n",
    "            remainder='drop'  # Drop ID columns\n",
    "        )\n",
    "        \n",
    "        # Fit the preprocessor\n",
    "        self.preprocessor.fit(X_copy)\n",
    "        \n",
    "        # Store feature names for later use\n",
    "        # Get feature names from transformers\n",
    "        numeric_feature_names = numeric_features\n",
    "        \n",
    "        try:\n",
    "            categorical_feature_names = (\n",
    "                self.preprocessor\n",
    "                .named_transformers_['cat']\n",
    "                .named_steps['encoder']\n",
    "                .get_feature_names_out(categorical_features)\n",
    "            )\n",
    "        except:\n",
    "            categorical_feature_names = []\n",
    "        \n",
    "        self.feature_names = list(numeric_feature_names) + list(categorical_feature_names)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Preprocessing pipeline fitted\")\n",
    "        print(f\"Final feature count: {len(self.feature_names)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform the data using fitted pipeline\"\"\"\n",
    "        if self.preprocessor is None:\n",
    "            raise ValueError(\"Pipeline not fitted. Call fit() first.\")\n",
    "        \n",
    "        X_transformed = self.preprocessor.transform(X)\n",
    "        \n",
    "        # Convert to DataFrame with proper column names\n",
    "        return pd.DataFrame(X_transformed, columns=self.feature_names, index=X.index)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "    \n",
    "    def get_feature_importance_mapping(self):\n",
    "        \"\"\"Get mapping of original to transformed features\"\"\"\n",
    "        return {\n",
    "            'feature_names': self.feature_names,\n",
    "            'outlier_bounds': self.outlier_bounds\n",
    "        }\n",
    "\n",
    "# Apply the preprocessing pipeline\n",
    "print(\"üîß PROFESSIONAL PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize and fit the preprocessor\n",
    "preprocessor = MLPreprocessor(handle_outliers=True, outlier_method='iqr')\n",
    "\n",
    "# Exclude customer_id for preprocessing\n",
    "feature_columns = [col for col in df_raw.columns if col != 'customer_id']\n",
    "X_raw = df_raw[feature_columns]\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = preprocessor.fit_transform(X_raw)\n",
    "\n",
    "print(f\"\\nüìä Transformation Results:\")\n",
    "print(f\"Original shape: {X_raw.shape}\")\n",
    "print(f\"Processed shape: {X_processed.shape}\")\n",
    "print(f\"Features created: {list(X_processed.columns)}\")\n",
    "\n",
    "# Validate the processed data\n",
    "validate_dataset(X_processed, \"Processed Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Checkpoint 2: Data Preprocessing\n",
    "\n",
    "**Assessment Questions (5 minutes):**\n",
    "\n",
    "1. Why is KNN imputation often better than mean/median imputation?\n",
    "2. What is data leakage and how does proper train/test splitting prevent it?\n",
    "3. When would you use RobustScaler instead of StandardScaler?\n",
    "4. What are the risks of dropping rows with missing values?\n",
    "\n",
    "**Practical Exercise:**\n",
    "Modify the preprocessing pipeline to:\n",
    "- Use different imputation strategies for different columns\n",
    "- Add polynomial features for numeric variables\n",
    "- Implement custom outlier handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Model Evaluation & Validation\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 40 minutes\n",
    "\n",
    "**Learning objectives:**\n",
    "- Implement robust cross-validation strategies\n",
    "- Understand bias-variance tradeoff\n",
    "- Master hyperparameter optimization\n",
    "- Detect overfitting and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cross-Validation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced cross-validation and model evaluation\n",
    "\n",
    "def comprehensive_model_evaluation(X, y, models, cv_strategy='stratified', n_splits=5, test_size=0.2):\n",
    "    \"\"\"Comprehensive model evaluation with multiple metrics\"\"\"\n",
    "    \n",
    "    print(\"üéØ COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Determine if this is classification or regression\n",
    "    is_classification = len(np.unique(y)) < 20 and y.dtype in ['int64', 'object', 'bool']\n",
    "    \n",
    "    print(f\"Problem type: {'Classification' if is_classification else 'Regression'}\")\n",
    "    print(f\"Cross-validation: {cv_strategy} {n_splits}-fold\")\n",
    "    \n",
    "    # Split data for final evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=RANDOM_STATE, \n",
    "        stratify=y if is_classification else None\n",
    "    )\n",
    "    \n",
    "    # Choose cross-validation strategy\n",
    "    if is_classification:\n",
    "        if cv_strategy == 'stratified':\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            cv = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro', 'roc_auc_ovr']\n",
    "    else:\n",
    "        cv = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring_metrics = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüîç Evaluating {name}...\")\n",
    "        \n",
    "        model_results = {'name': name}\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        for metric in scoring_metrics:\n",
    "            try:\n",
    "                scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=metric)\n",
    "                model_results[metric] = {\n",
    "                    'mean': scores.mean(),\n",
    "                    'std': scores.std(),\n",
    "                    'scores': scores\n",
    "                }\n",
    "                print(f\"  {metric}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {metric}: Error - {e}\")\n",
    "                model_results[metric] = {'mean': np.nan, 'std': np.nan, 'scores': []}\n",
    "        \n",
    "        # Final model evaluation on test set\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        if is_classification:\n",
    "            model_results['test_accuracy'] = accuracy_score(y_test, y_pred)\n",
    "            model_results['test_precision'] = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            model_results['test_recall'] = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            model_results['test_f1'] = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            model_results['test_mse'] = mean_squared_error(y_test, y_pred)\n",
    "            model_results['test_mae'] = mean_absolute_error(y_test, y_pred)\n",
    "            model_results['test_r2'] = r2_score(y_test, y_pred)\n",
    "        \n",
    "        model_results['model'] = model\n",
    "        model_results['y_test'] = y_test\n",
    "        model_results['y_pred'] = y_pred\n",
    "        \n",
    "        results[name] = model_results\n",
    "    \n",
    "    return results, X_test\n",
    "\n",
    "# Create target variable for demonstration\n",
    "# Customer lifetime value prediction (regression)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Create a realistic target based on features\n",
    "def create_target_variable(df):\n",
    "    \"\"\"Create realistic target variables\"\"\"\n",
    "    \n",
    "    # Customer Lifetime Value (CLV) - Regression target\n",
    "    base_clv = 1000\n",
    "    \n",
    "    # Impact of various factors on CLV\n",
    "    income_factor = np.log1p(df['income'].fillna(df['income'].median())) / 10\n",
    "    age_factor = np.where(df['age'] > 50, 1.2, np.where(df['age'] < 30, 0.8, 1.0))\n",
    "    spending_factor = np.log1p(df['monthly_spend']) / 2\n",
    "    loyalty_factor = np.log1p(df['num_purchases']) * 50\n",
    "    rating_factor = df['customer_rating'].fillna(3) * 100\n",
    "    \n",
    "    clv = (base_clv + income_factor + loyalty_factor + rating_factor) * age_factor + spending_factor\n",
    "    clv += np.random.normal(0, 200, len(df))  # Add noise\n",
    "    clv = np.maximum(clv, 100)  # Ensure positive values\n",
    "    \n",
    "    # High-value customer (binary classification target)\n",
    "    high_value = (clv > clv.quantile(0.7)).astype(int)\n",
    "    \n",
    "    return clv, high_value\n",
    "\n",
    "# Create targets\n",
    "clv_target, high_value_target = create_target_variable(df_raw)\n",
    "\n",
    "print(\"üéØ Target Variables Created:\")\n",
    "print(f\"CLV (regression): Mean={clv_target.mean():.0f}, Std={clv_target.std():.0f}\")\n",
    "print(f\"High-value customer (classification): {high_value_target.mean():.1%} positive class\")\n",
    "\n",
    "# Define models for evaluation\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'Neural Network': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "classification_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(probability=True, random_state=RANDOM_STATE),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Evaluate regression models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGRESSION EVALUATION (Customer Lifetime Value)\")\n",
    "regression_results, X_test_reg = comprehensive_model_evaluation(\n",
    "    X_processed, clv_target, regression_models, cv_strategy='standard'\n",
    ")\n",
    "\n",
    "# Evaluate classification models  \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION EVALUATION (High-Value Customer)\")\n",
    "classification_results, X_test_clf = comprehensive_model_evaluation(\n",
    "    X_processed, high_value_target, classification_models, cv_strategy='stratified'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Learning Curves & Bias-Variance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced learning curves and bias-variance analysis\n",
    "\n",
    "def plot_learning_curves(models, X, y, title_suffix=\"\"):\n",
    "    \"\"\"Plot learning curves to diagnose overfitting/underfitting\"\"\"\n",
    "    \n",
    "    n_models = len(models)\n",
    "    fig, axes = plt.subplots(2, (n_models + 1) // 2, figsize=(15, 8))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (name, model) in enumerate(models.items()):\n",
    "        # Calculate learning curves\n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            model, X, y, cv=5, n_jobs=-1, \n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        \n",
    "        # Calculate means and standard deviations\n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        # Plot learning curves\n",
    "        axes[idx].plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "        axes[idx].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "        \n",
    "        axes[idx].plot(train_sizes, val_mean, 'o-', color='red', label='Validation score')\n",
    "        axes[idx].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "        \n",
    "        axes[idx].set_title(f'{name} Learning Curve')\n",
    "        axes[idx].set_xlabel('Training Set Size')\n",
    "        axes[idx].set_ylabel('Score')\n",
    "        axes[idx].legend(loc='best')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Analyze bias-variance\n",
    "        final_train_score = train_mean[-1]\n",
    "        final_val_score = val_mean[-1]\n",
    "        gap = final_train_score - final_val_score\n",
    "        \n",
    "        # Add diagnosis text\n",
    "        if gap > 0.1:\n",
    "            diagnosis = \"High Variance (Overfitting)\"\n",
    "            color = 'red'\n",
    "        elif final_val_score < 0.7:  # Assuming scores are 0-1\n",
    "            diagnosis = \"High Bias (Underfitting)\"\n",
    "            color = 'orange'\n",
    "        else:\n",
    "            diagnosis = \"Good Balance\"\n",
    "            color = 'green'\n",
    "        \n",
    "        axes[idx].text(0.05, 0.95, diagnosis, transform=axes[idx].transAxes, \n",
    "                      bbox=dict(boxstyle='round', facecolor=color, alpha=0.3),\n",
    "                      verticalalignment='top')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(n_models, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'Learning Curves Analysis {title_suffix}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation_curves(model, X, y, param_name, param_range, title=\"\"):\n",
    "    \"\"\"Plot validation curves for hyperparameter tuning\"\"\"\n",
    "    \n",
    "    train_scores, val_scores = validation_curve(\n",
    "        model, X, y, param_name=param_name, param_range=param_range,\n",
    "        cv=5, scoring='accuracy' if len(np.unique(y)) < 20 else 'r2'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(param_range, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    \n",
    "    plt.plot(param_range, val_mean, 'o-', color='red', label='Validation score')\n",
    "    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    plt.title(f'Validation Curve: {title}')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find optimal parameter\n",
    "    optimal_idx = np.argmax(val_mean)\n",
    "    optimal_param = param_range[optimal_idx]\n",
    "    plt.axvline(optimal_param, color='green', linestyle='--', alpha=0.7, \n",
    "                label=f'Optimal: {optimal_param}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_param\n",
    "\n",
    "# Plot learning curves for classification models\n",
    "selected_clf_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(random_state=RANDOM_STATE),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "plot_learning_curves(selected_clf_models, X_processed, high_value_target, \"(Classification)\")\n",
    "\n",
    "# Plot learning curves for regression models\n",
    "selected_reg_models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "plot_learning_curves(selected_reg_models, X_processed, clv_target, \"(Regression)\")\n",
    "\n",
    "# Validation curves for hyperparameter tuning\n",
    "print(\"\\nüîß Hyperparameter Optimization Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Random Forest n_estimators\n",
    "rf_param_range = [10, 25, 50, 100, 200, 300]\n",
    "optimal_n_estimators = plot_validation_curves(\n",
    "    RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    X_processed, high_value_target,\n",
    "    'n_estimators', rf_param_range,\n",
    "    'Random Forest - Number of Estimators'\n",
    ")\n",
    "\n",
    "print(f\"Optimal n_estimators for Random Forest: {optimal_n_estimators}\")\n",
    "\n",
    "# Ridge regression alpha\n",
    "ridge_param_range = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "optimal_alpha = plot_validation_curves(\n",
    "    Ridge(random_state=RANDOM_STATE),\n",
    "    X_processed, clv_target,\n",
    "    'alpha', ridge_param_range,\n",
    "    'Ridge Regression - Alpha (Regularization)'\n",
    ")\n",
    "\n",
    "print(f\"Optimal alpha for Ridge Regression: {optimal_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Advanced Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional hyperparameter optimization\n",
    "\n",
    "def advanced_hyperparameter_tuning(X, y, model_class, param_distributions, \n",
    "                                 search_type='random', n_iter=50, cv=5):\n",
    "    \"\"\"Advanced hyperparameter optimization with multiple strategies\"\"\"\n",
    "    \n",
    "    print(f\"\\nüéØ {search_type.upper()} HYPERPARAMETER SEARCH\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Model: {model_class.__name__}\")\n",
    "    print(f\"Search iterations: {n_iter}\")\n",
    "    print(f\"Cross-validation folds: {cv}\")\n",
    "    \n",
    "    # Determine scoring metric\n",
    "    is_classification = len(np.unique(y)) < 20\n",
    "    scoring = 'accuracy' if is_classification else 'r2'\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE,\n",
    "        stratify=y if is_classification else None\n",
    "    )\n",
    "    \n",
    "    # Initialize search\n",
    "    if search_type == 'grid':\n",
    "        search = GridSearchCV(\n",
    "            model_class(random_state=RANDOM_STATE),\n",
    "            param_distributions,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "    else:  # random search\n",
    "        search = RandomizedSearchCV(\n",
    "            model_class(random_state=RANDOM_STATE),\n",
    "            param_distributions,\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    # Perform search\n",
    "    start_time = datetime.now()\n",
    "    search.fit(X_train, y_train)\n",
    "    search_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Results\n",
    "    print(f\"\\n‚è±Ô∏è Search completed in {search_time:.1f} seconds\")\n",
    "    print(f\"üèÜ Best cross-validation score: {search.best_score_:.4f}\")\n",
    "    print(f\"üéØ Best parameters: {search.best_params_}\")\n",
    "    \n",
    "    # Test the best model\n",
    "    best_model = search.best_estimator_\n",
    "    test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    if is_classification:\n",
    "        test_score = accuracy_score(y_test, test_pred)\n",
    "        print(f\"üéØ Test accuracy: {test_score:.4f}\")\n",
    "    else:\n",
    "        test_score = r2_score(y_test, test_pred)\n",
    "        print(f\"üéØ Test R¬≤ score: {test_score:.4f}\")\n",
    "    \n",
    "    # Feature importance (if available)\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüîç Top 5 Feature Importances:\")\n",
    "        for _, row in feature_importance.head().iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_params': search.best_params_,\n",
    "        'best_score': search.best_score_,\n",
    "        'test_score': test_score,\n",
    "        'search_time': search_time,\n",
    "        'cv_results': search.cv_results_\n",
    "    }\n",
    "\n",
    "# Define parameter distributions for different models\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "svm_param_dist = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "mlp_param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50), (200, 100), (100, 100, 50)],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning for classification\n",
    "print(\"üî¨ CLASSIFICATION HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Random Forest Classification\n",
    "rf_clf_results = advanced_hyperparameter_tuning(\n",
    "    X_processed, high_value_target,\n",
    "    RandomForestClassifier,\n",
    "    rf_param_dist,\n",
    "    search_type='random',\n",
    "    n_iter=30\n",
    ")\n",
    "\n",
    "# SVM Classification\n",
    "svm_clf_results = advanced_hyperparameter_tuning(\n",
    "    X_processed, high_value_target,\n",
    "    SVC,\n",
    "    svm_param_dist,\n",
    "    search_type='random',\n",
    "    n_iter=20\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning for regression\n",
    "print(\"\\n\\nüî¨ REGRESSION HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Random Forest Regression\n",
    "rf_reg_results = advanced_hyperparameter_tuning(\n",
    "    X_processed, clv_target,\n",
    "    RandomForestRegressor,\n",
    "    rf_param_dist,\n",
    "    search_type='random',\n",
    "    n_iter=30\n",
    ")\n",
    "\n",
    "# Compare optimization results\n",
    "print(\"\\n\\nüìä HYPERPARAMETER OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Random Forest (Clf)',\n",
    "        'Best CV Score': rf_clf_results['best_score'],\n",
    "        'Test Score': rf_clf_results['test_score'],\n",
    "        'Search Time (s)': rf_clf_results['search_time']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'SVM (Clf)',\n",
    "        'Best CV Score': svm_clf_results['best_score'],\n",
    "        'Test Score': svm_clf_results['test_score'],\n",
    "        'Search Time (s)': svm_clf_results['search_time']\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Random Forest (Reg)',\n",
    "        'Best CV Score': rf_reg_results['best_score'],\n",
    "        'Test Score': rf_reg_results['test_score'],\n",
    "        'Search Time (s)': rf_reg_results['search_time']\n",
    "    }\n",
    "])\n",
    "\n",
    "print(results_summary.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Checkpoint 3: Model Evaluation\n",
    "\n",
    "**Assessment Questions (5 minutes):**\n",
    "\n",
    "1. What does a large gap between training and validation scores indicate?\n",
    "2. When would you use RandomizedSearchCV instead of GridSearchCV?\n",
    "3. How do you interpret a learning curve that shows both training and validation scores plateauing at a low level?\n",
    "4. What are the trade-offs between different cross-validation strategies?\n",
    "\n",
    "**Practical Exercise:**\n",
    "- Implement nested cross-validation for unbiased model selection\n",
    "- Create custom scoring functions for business metrics\n",
    "- Analyze the impact of different train/test split ratios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
