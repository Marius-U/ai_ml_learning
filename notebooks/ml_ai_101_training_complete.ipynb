{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning & AI 101: Complete Professional Training\n",
    "\n",
    "üéØ **Welcome to the most comprehensive ML/AI training for data professionals!**\n",
    "\n",
    "This enhanced notebook transforms beginners into competent ML practitioners through:\n",
    "- **Systematic skill building** with measurable learning outcomes\n",
    "- **Real-world applications** using actual data sources and deployment techniques\n",
    "- **Industry best practices** including MLOps, testing, and production considerations\n",
    "- **Interactive assessments** to validate your progress\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "\n",
    "By completing this training, you will:\n",
    "\n",
    "1. **Master data preprocessing pipelines** for production-ready ML systems\n",
    "2. **Implement robust model evaluation** with proper validation strategies\n",
    "3. **Build end-to-end ML applications** with real data sources and deployment\n",
    "4. **Apply MLOps principles** for model versioning, monitoring, and maintenance\n",
    "5. **Handle ethical considerations** including bias detection and fairness metrics\n",
    "6. **Debug common ML issues** and optimize model performance\n",
    "\n",
    "**Estimated completion time:** 8-12 hours (can be completed in modules)\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Prerequisites & Environment Setup\n",
    "\n",
    "### Required Knowledge\n",
    "- [ ] Basic Python programming (functions, classes, data structures)\n",
    "- [ ] Elementary statistics (mean, variance, distributions)\n",
    "- [ ] High school mathematics (algebra, basic calculus helpful but not required)\n",
    "\n",
    "### Success Criteria\n",
    "- [ ] Complete all checkpoint assessments with 70%+ scores\n",
    "- [ ] Successfully implement at least 2 end-to-end projects\n",
    "- [ ] Demonstrate ability to debug and optimize ML models\n",
    "\n",
    "Let's verify your environment and begin your professional ML journey! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Validation\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 15 minutes\n",
    "\n",
    "**Learning objectives:**\n",
    "- Set up a reproducible ML environment\n",
    "- Understand version management for ML projects\n",
    "- Implement proper random seed management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup with version tracking and reproducibility\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Core libraries with version checking\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Machine Learning - Core\n",
    "import sklearn\n",
    "from sklearn.datasets import (\n",
    "    load_iris, load_wine, load_breast_cancer, \n",
    "    make_classification, make_regression, make_blobs\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV, \n",
    "    RandomizedSearchCV, validation_curve, learning_curve,\n",
    "    StratifiedKFold, KFold\n",
    ")\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, \n",
    "    LabelEncoder, OneHotEncoder, PolynomialFeatures\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    GradientBoostingClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    silhouette_score, adjusted_rand_score\n",
    ")\n",
    "\n",
    "# Advanced libraries\n",
    "try:\n",
    "    import joblib\n",
    "    HAS_JOBLIB = True\n",
    "except ImportError:\n",
    "    HAS_JOBLIB = False\n",
    "    \n",
    "try:\n",
    "    import requests\n",
    "    HAS_REQUESTS = True\n",
    "except ImportError:\n",
    "    HAS_REQUESTS = False\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Global random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Environment validation\n",
    "print(\"üîß ML ENVIRONMENT VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "\n",
    "print(\"\\nüì¶ Optional Libraries:\")\n",
    "print(f\"Joblib available: {'‚úÖ' if HAS_JOBLIB else '‚ùå (pip install joblib)'}\")\n",
    "print(f\"Requests available: {'‚úÖ' if HAS_REQUESTS else '‚ùå (pip install requests)'}\")\n",
    "\n",
    "print(f\"\\nüé≤ Random state set to: {RANDOM_STATE}\")\n",
    "print(f\"üìÖ Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n‚úÖ Environment ready for ML training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Checkpoint 1: Environment Validation\n",
    "\n",
    "**Quick Assessment (2 minutes):**\n",
    "\n",
    "1. What is the purpose of setting a random state in ML projects?\n",
    "2. Why do we suppress warnings in production ML code?\n",
    "3. What happens if you don't manage package versions in ML projects?\n",
    "\n",
    "<details>\n",
    "<summary>Click for answers</summary>\n",
    "\n",
    "1. **Random state ensures reproducibility** - same results across runs and different environments\n",
    "2. **Suppress warnings to avoid clutter** in production logs, but keep them during development\n",
    "3. **Version mismatches can cause** model performance changes, crashes, or different results\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Fundamentals & Professional Preprocessing\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 45 minutes\n",
    "\n",
    "**Learning objectives:**\n",
    "- Master production-ready data preprocessing pipelines\n",
    "- Handle missing data with advanced strategies\n",
    "- Implement feature engineering and validation\n",
    "- Understand data leakage and prevention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Advanced Data Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional data validation and quality assessment\n",
    "def validate_dataset(df, name=\"Dataset\"):\n",
    "    \"\"\"Comprehensive data validation function\"\"\"\n",
    "    print(f\"\\nüìä {name} Validation Report\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Missing data analysis\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing': missing,\n",
    "        'Percentage': missing_pct\n",
    "    }).sort_values('Missing', ascending=False)\n",
    "    \n",
    "    if missing.sum() > 0:\n",
    "        print(\"\\n‚ö†Ô∏è Missing Data:\")\n",
    "        print(missing_df[missing_df['Missing'] > 0])\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing data found\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate rows: {duplicates} ({duplicates/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Numeric column statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìà Numeric columns: {len(numeric_cols)}\")\n",
    "        print(\"Range check:\")\n",
    "        for col in numeric_cols:\n",
    "            print(f\"  {col}: [{df[col].min():.3f}, {df[col].max():.3f}]\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Create a comprehensive synthetic dataset for demonstration\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "n_samples = 1000\n",
    "synthetic_data = {\n",
    "    # Demographic features\n",
    "    'customer_id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'income': np.random.lognormal(10.5, 0.8, n_samples),  # More realistic income distribution\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_samples),\n",
    "    \n",
    "    # Behavioral features\n",
    "    'monthly_spend': np.random.gamma(2, 50, n_samples),\n",
    "    'num_purchases': np.random.poisson(8, n_samples),\n",
    "    'days_since_last_purchase': np.random.exponential(10, n_samples),\n",
    "    'customer_rating': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.05, 0.1, 0.2, 0.4, 0.25]),\n",
    "    \n",
    "    # Technical features\n",
    "    'website_visits': np.random.negative_binomial(10, 0.3, n_samples),\n",
    "    'mobile_app_usage': np.random.beta(2, 5, n_samples) * 100,  # Percentage\n",
    "    'email_open_rate': np.random.beta(3, 7, n_samples),\n",
    "}\n",
    "\n",
    "# Add missing values realistically (missing not at random)\n",
    "df_raw = pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Income missing for younger customers (survey bias)\n",
    "young_indices = df_raw[df_raw['age'] < 25].index\n",
    "missing_indices = np.random.choice(young_indices, size=min(len(young_indices)//3, 30), replace=False)\n",
    "df_raw.loc[missing_indices, 'income'] = np.nan\n",
    "\n",
    "# Rating missing for customers with very few purchases\n",
    "low_purchase_indices = df_raw[df_raw['num_purchases'] <= 2].index\n",
    "missing_indices = np.random.choice(low_purchase_indices, size=min(len(low_purchase_indices)//2, 20), replace=False)\n",
    "df_raw.loc[missing_indices, 'customer_rating'] = np.nan\n",
    "\n",
    "# App usage missing for older customers\n",
    "old_indices = df_raw[df_raw['age'] > 65].index\n",
    "missing_indices = np.random.choice(old_indices, size=min(len(old_indices)//2, 25), replace=False)\n",
    "df_raw.loc[missing_indices, 'mobile_app_usage'] = np.nan\n",
    "\n",
    "# Add some extreme outliers\n",
    "outlier_indices = np.random.choice(df_raw.index, 20, replace=False)\n",
    "df_raw.loc[outlier_indices, 'monthly_spend'] *= 10  # Very high spenders\n",
    "\n",
    "# Validate the dataset\n",
    "validation_report = validate_dataset(df_raw, \"Customer Dataset\")\n",
    "\n",
    "print(\"\\nüéØ First 5 rows:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Production-Ready Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional preprocessing pipeline using sklearn\n",
    "\n",
    "class MLPreprocessor:\n",
    "    \"\"\"Production-ready preprocessing pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, handle_outliers=True, outlier_method='iqr'):\n",
    "        self.handle_outliers = handle_outliers\n",
    "        self.outlier_method = outlier_method\n",
    "        self.preprocessor = None\n",
    "        self.feature_names = None\n",
    "        self.outlier_bounds = {}\n",
    "        \n",
    "    def detect_outliers(self, X, column, method='iqr'):\n",
    "        \"\"\"Detect outliers using IQR or z-score method\"\"\"\n",
    "        if method == 'iqr':\n",
    "            Q1 = X[column].quantile(0.25)\n",
    "            Q3 = X[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "        else:  # z-score\n",
    "            mean = X[column].mean()\n",
    "            std = X[column].std()\n",
    "            lower_bound = mean - 3 * std\n",
    "            upper_bound = mean + 3 * std\n",
    "        \n",
    "        self.outlier_bounds[column] = (lower_bound, upper_bound)\n",
    "        outliers = (X[column] < lower_bound) | (X[column] > upper_bound)\n",
    "        return outliers\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the preprocessing pipeline\"\"\"\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Separate numeric and categorical columns\n",
    "        numeric_features = X_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_features = X_copy.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        \n",
    "        # Remove ID columns if present\n",
    "        id_columns = [col for col in numeric_features if 'id' in col.lower()]\n",
    "        numeric_features = [col for col in numeric_features if col not in id_columns]\n",
    "        \n",
    "        print(f\"Identified features:\")\n",
    "        print(f\"  Numeric: {len(numeric_features)} - {numeric_features}\")\n",
    "        print(f\"  Categorical: {len(categorical_features)} - {categorical_features}\")\n",
    "        print(f\"  ID columns (excluded): {id_columns}\")\n",
    "        \n",
    "        # Handle outliers in numeric features\n",
    "        if self.handle_outliers:\n",
    "            print(f\"\\nüéØ Outlier Detection ({self.outlier_method} method):\")\n",
    "            for col in numeric_features:\n",
    "                outliers = self.detect_outliers(X_copy, col, self.outlier_method)\n",
    "                outlier_count = outliers.sum()\n",
    "                if outlier_count > 0:\n",
    "                    print(f\"  {col}: {outlier_count} outliers ({outlier_count/len(X_copy)*100:.1f}%)\")\n",
    "        \n",
    "        # Create preprocessing pipelines\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', KNNImputer(n_neighbors=5)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "            ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        # Combine transformers\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ],\n",
    "            remainder='drop'  # Drop ID columns\n",
    "        )\n",
    "        \n",
    "        # Fit the preprocessor\n",
    "        self.preprocessor.fit(X_copy)\n",
    "        \n",
    "        # Store feature names for later use\n",
    "        numeric_feature_names = numeric_features\n",
    "        \n",
    "        try:\n",
    "            categorical_feature_names = (\n",
    "                self.preprocessor\n",
    "                .named_transformers_['cat']\n",
    "                .named_steps['encoder']\n",
    "                .get_feature_names_out(categorical_features)\n",
    "            )\n",
    "        except:\n",
    "            categorical_feature_names = []\n",
    "        \n",
    "        self.feature_names = list(numeric_feature_names) + list(categorical_feature_names)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Preprocessing pipeline fitted\")\n",
    "        print(f\"Final feature count: {len(self.feature_names)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform the data using fitted pipeline\"\"\"\n",
    "        if self.preprocessor is None:\n",
    "            raise ValueError(\"Pipeline not fitted. Call fit() first.\")\n",
    "        \n",
    "        X_transformed = self.preprocessor.transform(X)\n",
    "        \n",
    "        # Convert to DataFrame with proper column names\n",
    "        return pd.DataFrame(X_transformed, columns=self.feature_names, index=X.index)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "    \n",
    "    def get_feature_importance_mapping(self):\n",
    "        \"\"\"Get mapping of original to transformed features\"\"\"\n",
    "        return {\n",
    "            'feature_names': self.feature_names,\n",
    "            'outlier_bounds': self.outlier_bounds\n",
    "        }\n",
    "\n",
    "# Apply the preprocessing pipeline\n",
    "print(\"üîß PROFESSIONAL PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize and fit the preprocessor\n",
    "preprocessor = MLPreprocessor(handle_outliers=True, outlier_method='iqr')\n",
    "\n",
    "# Exclude customer_id for preprocessing\n",
    "feature_columns = [col for col in df_raw.columns if col != 'customer_id']\n",
    "X_raw = df_raw[feature_columns]\n",
    "\n",
    "# Fit and transform\n",
    "X_processed = preprocessor.fit_transform(X_raw)\n",
    "\n",
    "print(f\"\\nüìä Transformation Results:\")\n",
    "print(f\"Original shape: {X_raw.shape}\")\n",
    "print(f\"Processed shape: {X_processed.shape}\")\n",
    "print(f\"Features created: {list(X_processed.columns)}\")\n",
    "\n",
    "# Validate the processed data\n",
    "validate_dataset(X_processed, \"Processed Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Checkpoint 2: Data Preprocessing\n",
    "\n",
    "**Assessment Questions (5 minutes):**\n",
    "\n",
    "1. Why is KNN imputation often better than mean/median imputation?\n",
    "2. What is data leakage and how does proper train/test splitting prevent it?\n",
    "3. When would you use RobustScaler instead of StandardScaler?\n",
    "4. What are the risks of dropping rows with missing values?\n",
    "\n",
    "**Practical Exercise:**\n",
    "Modify the preprocessing pipeline to:\n",
    "- Use different imputation strategies for different columns\n",
    "- Add polynomial features for numeric variables\n",
    "- Implement custom outlier handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Model Evaluation & Validation\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 40 minutes\n",
    "\n",
    "**Learning objectives:**\n",
    "- Implement robust cross-validation strategies\n",
    "- Understand bias-variance tradeoff\n",
    "- Master hyperparameter optimization\n",
    "- Detect overfitting and model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variables for demonstration\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "def create_target_variable(df):\n",
    "    \"\"\"Create realistic target variables\"\"\"\n",
    "    \n",
    "    # Customer Lifetime Value (CLV) - Regression target\n",
    "    base_clv = 1000\n",
    "    \n",
    "    # Impact of various factors on CLV\n",
    "    income_factor = np.log1p(df['income'].fillna(df['income'].median())) / 10\n",
    "    age_factor = np.where(df['age'] > 50, 1.2, np.where(df['age'] < 30, 0.8, 1.0))\n",
    "    spending_factor = np.log1p(df['monthly_spend']) / 2\n",
    "    loyalty_factor = np.log1p(df['num_purchases']) * 50\n",
    "    rating_factor = df['customer_rating'].fillna(3) * 100\n",
    "    \n",
    "    clv = (base_clv + income_factor + loyalty_factor + rating_factor) * age_factor + spending_factor\n",
    "    clv += np.random.normal(0, 200, len(df))  # Add noise\n",
    "    clv = np.maximum(clv, 100)  # Ensure positive values\n",
    "    \n",
    "    # High-value customer (binary classification target)\n",
    "    high_value = (clv > clv.quantile(0.7)).astype(int)\n",
    "    \n",
    "    return clv, high_value\n",
    "\n",
    "# Create targets\n",
    "clv_target, high_value_target = create_target_variable(df_raw)\n",
    "\n",
    "print(\"üéØ Target Variables Created:\")\n",
    "print(f\"CLV (regression): Mean={clv_target.mean():.0f}, Std={clv_target.std():.0f}\")\n",
    "print(f\"High-value customer (classification): {high_value_target.mean():.1%} positive class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced model evaluation framework\n",
    "\n",
    "def comprehensive_model_evaluation(X, y, models, cv_strategy='stratified', n_splits=5, test_size=0.2):\n",
    "    \"\"\"Comprehensive model evaluation with multiple metrics\"\"\"\n",
    "    \n",
    "    print(\"üéØ COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Determine if this is classification or regression\n",
    "    is_classification = len(np.unique(y)) < 20 and y.dtype in ['int64', 'object', 'bool']\n",
    "    \n",
    "    print(f\"Problem type: {'Classification' if is_classification else 'Regression'}\")\n",
    "    print(f\"Cross-validation: {cv_strategy} {n_splits}-fold\")\n",
    "    \n",
    "    # Split data for final evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=RANDOM_STATE, \n",
    "        stratify=y if is_classification else None\n",
    "    )\n",
    "    \n",
    "    # Choose cross-validation strategy\n",
    "    if is_classification:\n",
    "        if cv_strategy == 'stratified':\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            cv = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "    else:\n",
    "        cv = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        scoring_metrics = ['neg_mean_squared_error', 'neg_mean_absolute_error', 'r2']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüîç Evaluating {name}...\")\n",
    "        \n",
    "        model_results = {'name': name}\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        for metric in scoring_metrics:\n",
    "            try:\n",
    "                scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=metric)\n",
    "                model_results[metric] = {\n",
    "                    'mean': scores.mean(),\n",
    "                    'std': scores.std(),\n",
    "                    'scores': scores\n",
    "                }\n",
    "                print(f\"  {metric}: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {metric}: Error - {str(e)[:50]}...\")\n",
    "                model_results[metric] = {'mean': np.nan, 'std': np.nan, 'scores': []}\n",
    "        \n",
    "        # Final model evaluation on test set\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        if is_classification:\n",
    "            model_results['test_accuracy'] = accuracy_score(y_test, y_pred)\n",
    "            model_results['test_precision'] = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            model_results['test_recall'] = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            model_results['test_f1'] = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        else:\n",
    "            model_results['test_mse'] = mean_squared_error(y_test, y_pred)\n",
    "            model_results['test_mae'] = mean_absolute_error(y_test, y_pred)\n",
    "            model_results['test_r2'] = r2_score(y_test, y_pred)\n",
    "        \n",
    "        model_results['model'] = model\n",
    "        model_results['y_test'] = y_test\n",
    "        model_results['y_pred'] = y_pred\n",
    "        \n",
    "        results[name] = model_results\n",
    "    \n",
    "    return results, X_test\n",
    "\n",
    "# Define models for evaluation\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=50, random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "classification_models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(probability=True, random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "# Evaluate regression models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGRESSION EVALUATION (Customer Lifetime Value)\")\n",
    "regression_results, X_test_reg = comprehensive_model_evaluation(\n",
    "    X_processed, clv_target, regression_models, cv_strategy='standard'\n",
    ")\n",
    "\n",
    "# Evaluate classification models  \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION EVALUATION (High-Value Customer)\")\n",
    "classification_results, X_test_clf = comprehensive_model_evaluation(\n",
    "    X_processed, high_value_target, classification_models, cv_strategy='stratified'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-World Data Integration & MLOps\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 50 minutes\n",
    "\n",
    "**Learning objectives:**\n",
    "- Connect to real data sources (APIs, databases, web scraping)\n",
    "- Implement model persistence and versioning\n",
    "- Apply MLOps principles for production deployments\n",
    "- Handle data quality monitoring and model drift detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world data integration strategies\n",
    "\n",
    "import sqlite3\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "class DataConnector:\n",
    "    \"\"\"Professional data connector for various sources\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.connection_cache = {}\n",
    "        self.request_session = None\n",
    "        if HAS_REQUESTS:\n",
    "            self.request_session = requests.Session()\n",
    "            self.request_session.headers.update({\n",
    "                'User-Agent': 'ML-Training-Notebook/1.0'\n",
    "            })\n",
    "    \n",
    "    def create_sample_database(self, db_path='sample_data.db'):\n",
    "        \"\"\"Create a sample SQLite database for demonstration\"\"\"\n",
    "        \n",
    "        print(f\"üóÑÔ∏è Creating sample database: {db_path}\")\n",
    "        \n",
    "        # Create connection\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create tables\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS customers (\n",
    "                customer_id INTEGER PRIMARY KEY,\n",
    "                name TEXT NOT NULL,\n",
    "                email TEXT UNIQUE,\n",
    "                registration_date DATE,\n",
    "                country TEXT,\n",
    "                subscription_tier TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS transactions (\n",
    "                transaction_id INTEGER PRIMARY KEY,\n",
    "                customer_id INTEGER,\n",
    "                amount REAL,\n",
    "                transaction_date DATETIME,\n",
    "                product_category TEXT,\n",
    "                FOREIGN KEY (customer_id) REFERENCES customers (customer_id)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Insert sample data\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        \n",
    "        # Generate customers\n",
    "        countries = ['USA', 'UK', 'Germany', 'France', 'Canada', 'Australia']\n",
    "        tiers = ['Basic', 'Premium', 'Enterprise']\n",
    "        \n",
    "        customers_data = []\n",
    "        for i in range(1, 101):  # 100 customers\n",
    "            customers_data.append((\n",
    "                i,\n",
    "                f\"Customer_{i:03d}\",\n",
    "                f\"customer{i:03d}@email.com\",\n",
    "                (datetime.now() - pd.Timedelta(days=np.random.randint(30, 365))).date(),\n",
    "                np.random.choice(countries),\n",
    "                np.random.choice(tiers, p=[0.5, 0.35, 0.15])\n",
    "            ))\n",
    "        \n",
    "        cursor.executemany(\n",
    "            'INSERT OR REPLACE INTO customers VALUES (?, ?, ?, ?, ?, ?)',\n",
    "            customers_data\n",
    "        )\n",
    "        \n",
    "        # Generate transactions\n",
    "        categories = ['Electronics', 'Clothing', 'Books', 'Home', 'Sports']\n",
    "        transactions_data = []\n",
    "        \n",
    "        for i in range(1, 301):  # 300 transactions\n",
    "            customer_id = np.random.randint(1, 101)\n",
    "            amount = np.random.lognormal(3, 1)  # Log-normal distribution for realistic amounts\n",
    "            transaction_date = datetime.now() - pd.Timedelta(days=np.random.randint(0, 90))\n",
    "            category = np.random.choice(categories)\n",
    "            \n",
    "            transactions_data.append((\n",
    "                i, customer_id, round(amount, 2), transaction_date, category\n",
    "            ))\n",
    "        \n",
    "        cursor.executemany(\n",
    "            'INSERT OR REPLACE INTO transactions VALUES (?, ?, ?, ?, ?)',\n",
    "            transactions_data\n",
    "        )\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        print(f\"‚úÖ Database created with {len(customers_data)} customers and {len(transactions_data)} transactions\")\n",
    "        return db_path\n",
    "    \n",
    "    def connect_to_database(self, db_path, query):\n",
    "        \"\"\"Connect to SQLite database and execute query\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"üóÑÔ∏è Connecting to database: {db_path}\")\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            \n",
    "            # Execute query and return DataFrame\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            conn.close()\n",
    "            \n",
    "            print(f\"‚úÖ Query executed successfully. Retrieved {len(df)} rows\")\n",
    "            return df\n",
    "            \n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"‚ùå Database error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def simulate_web_scraping(self, num_records=50):\n",
    "        \"\"\"Simulate web scraping (without actual scraping)\"\"\"\n",
    "        \n",
    "        print(f\"üï∑Ô∏è Simulating web scraping for {num_records} records...\")\n",
    "        \n",
    "        # Simulate realistic web-scraped data\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        \n",
    "        # Simulate product data from e-commerce site\n",
    "        products = []\n",
    "        categories = ['Electronics', 'Books', 'Clothing', 'Home & Garden', 'Sports']\n",
    "        brands = ['BrandA', 'BrandB', 'BrandC', 'BrandD', 'BrandE']\n",
    "        \n",
    "        for i in range(num_records):\n",
    "            # Simulate some missing data (realistic for web scraping)\n",
    "            rating = np.random.uniform(1, 5) if np.random.random() > 0.1 else None\n",
    "            price = np.random.lognormal(3, 0.8) if np.random.random() > 0.05 else None\n",
    "            \n",
    "            product = {\n",
    "                'product_id': f'P{i:04d}',\n",
    "                'name': f'Product {i}',\n",
    "                'category': np.random.choice(categories),\n",
    "                'brand': np.random.choice(brands) if np.random.random() > 0.15 else None,\n",
    "                'price': round(price, 2) if price else None,\n",
    "                'rating': round(rating, 1) if rating else None,\n",
    "                'num_reviews': np.random.poisson(50) if rating else 0,\n",
    "                'in_stock': np.random.choice([True, False], p=[0.85, 0.15]),\n",
    "                'scraped_date': datetime.now() - pd.Timedelta(hours=np.random.randint(0, 24))\n",
    "            }\n",
    "            products.append(product)\n",
    "        \n",
    "        df = pd.DataFrame(products)\n",
    "        \n",
    "        print(f\"‚úÖ Simulated scraping complete. Created dataset with shape {df.shape}\")\n",
    "        print(f\"Missing data: {df.isnull().sum().sum()} total missing values\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Initialize data connector\n",
    "data_connector = DataConnector()\n",
    "\n",
    "# Demonstrate different data sources\n",
    "print(\"üåç REAL-WORLD DATA INTEGRATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Database connection\n",
    "db_path = data_connector.create_sample_database()\n",
    "\n",
    "# Query customer data\n",
    "customer_query = '''\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.name,\n",
    "        c.country,\n",
    "        c.subscription_tier,\n",
    "        COUNT(t.transaction_id) as num_transactions,\n",
    "        SUM(t.amount) as total_spent,\n",
    "        AVG(t.amount) as avg_transaction\n",
    "    FROM customers c\n",
    "    LEFT JOIN transactions t ON c.customer_id = t.customer_id\n",
    "    GROUP BY c.customer_id\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "'''\n",
    "\n",
    "customer_data = data_connector.connect_to_database(db_path, customer_query)\n",
    "if customer_data is not None:\n",
    "    print(\"\\nüèÜ Top 10 customers by total spent:\")\n",
    "    print(customer_data.head())\n",
    "\n",
    "# 2. Web scraping simulation\n",
    "scraped_data = data_connector.simulate_web_scraping(30)\n",
    "print(\"\\nüï∑Ô∏è Sample scraped data:\")\n",
    "print(scraped_data.head())\n",
    "print(f\"\\nData quality check - Missing values per column:\")\n",
    "print(scraped_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Persistence & Production Deployment\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 30 minutes\n",
    "\n",
    "**Learning objectives:**\n",
    "- Save and load trained models\n",
    "- Version control for ML models\n",
    "- Create prediction APIs\n",
    "- Monitor model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model persistence and versioning\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Professional model management and versioning\"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir='models'):\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def save_model(self, model, model_name, version='1.0', metadata=None):\n",
    "        \"\"\"Save model with versioning and metadata\"\"\"\n",
    "        \n",
    "        # Create version directory\n",
    "        version_dir = self.model_dir / model_name / f\"v{version}\"\n",
    "        version_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = version_dir / 'model.pkl'\n",
    "        \n",
    "        if HAS_JOBLIB:\n",
    "            import joblib\n",
    "            joblib.dump(model, model_path)\n",
    "        else:\n",
    "            with open(model_path, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "        \n",
    "        # Save metadata\n",
    "        model_metadata = {\n",
    "            'model_name': model_name,\n",
    "            'version': version,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'model_type': type(model).__name__,\n",
    "            'sklearn_version': sklearn.__version__,\n",
    "            'python_version': sys.version,\n",
    "        }\n",
    "        \n",
    "        if metadata:\n",
    "            model_metadata.update(metadata)\n",
    "        \n",
    "        metadata_path = version_dir / 'metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(model_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved: {model_path}\")\n",
    "        print(f\"üìù Metadata saved: {metadata_path}\")\n",
    "        \n",
    "        return str(model_path)\n",
    "    \n",
    "    def load_model(self, model_name, version='1.0'):\n",
    "        \"\"\"Load model with specified version\"\"\"\n",
    "        \n",
    "        model_path = self.model_dir / model_name / f\"v{version}\" / 'model.pkl'\n",
    "        metadata_path = self.model_dir / model_name / f\"v{version}\" / 'metadata.json'\n",
    "        \n",
    "        if not model_path.exists():\n",
    "            raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
    "        \n",
    "        # Load model\n",
    "        if HAS_JOBLIB:\n",
    "            import joblib\n",
    "            model = joblib.load(model_path)\n",
    "        else:\n",
    "            with open(model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata = {}\n",
    "        if metadata_path.exists():\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded: {model_path}\")\n",
    "        print(f\"üìä Model type: {metadata.get('model_type', 'Unknown')}\")\n",
    "        print(f\"üìÖ Created: {metadata.get('created_at', 'Unknown')}\")\n",
    "        \n",
    "        return model, metadata\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List all available models and versions\"\"\"\n",
    "        \n",
    "        models = []\n",
    "        for model_dir in self.model_dir.iterdir():\n",
    "            if model_dir.is_dir():\n",
    "                for version_dir in model_dir.iterdir():\n",
    "                    if version_dir.is_dir() and version_dir.name.startswith('v'):\n",
    "                        metadata_path = version_dir / 'metadata.json'\n",
    "                        metadata = {}\n",
    "                        if metadata_path.exists():\n",
    "                            with open(metadata_path, 'r') as f:\n",
    "                                metadata = json.load(f)\n",
    "                        \n",
    "                        models.append({\n",
    "                            'name': model_dir.name,\n",
    "                            'version': version_dir.name[1:],  # Remove 'v' prefix\n",
    "                            'type': metadata.get('model_type', 'Unknown'),\n",
    "                            'created': metadata.get('created_at', 'Unknown')\n",
    "                        })\n",
    "        \n",
    "        return models\n",
    "\n",
    "# Demonstrate model persistence\n",
    "print(\"üíæ MODEL PERSISTENCE & VERSIONING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = ModelManager()\n",
    "\n",
    "# Train and save a simple model\n",
    "best_model = RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE)\n",
    "best_model.fit(X_processed, high_value_target)\n",
    "\n",
    "# Calculate model performance for metadata\n",
    "y_pred = best_model.predict(X_processed)\n",
    "accuracy = accuracy_score(high_value_target, y_pred)\n",
    "\n",
    "# Save model with metadata\n",
    "model_metadata = {\n",
    "    'accuracy': accuracy,\n",
    "    'n_features': X_processed.shape[1],\n",
    "    'n_samples': X_processed.shape[0],\n",
    "    'target_distribution': high_value_target.value_counts().to_dict(),\n",
    "    'feature_names': list(X_processed.columns)\n",
    "}\n",
    "\n",
    "model_path = model_manager.save_model(\n",
    "    model=best_model,\n",
    "    model_name='customer_value_classifier',\n",
    "    version='1.0',\n",
    "    metadata=model_metadata\n",
    ")\n",
    "\n",
    "# List available models\n",
    "print(\"\\nüìã Available Models:\")\n",
    "available_models = model_manager.list_models()\n",
    "for model in available_models:\n",
    "    print(f\"  {model['name']} v{model['version']} ({model['type']}) - {model['created'][:10]}\")\n",
    "\n",
    "# Load model back\n",
    "print(\"\\nüîÑ Loading Model:\")\n",
    "loaded_model, loaded_metadata = model_manager.load_model('customer_value_classifier', '1.0')\n",
    "\n",
    "# Test loaded model\n",
    "test_predictions = loaded_model.predict(X_processed[:5])\n",
    "print(f\"\\nüß™ Test predictions on first 5 samples: {test_predictions}\")\n",
    "print(f\"‚úÖ Model persistence working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Project: End-to-End ML Pipeline\n",
    "\n",
    "**‚è±Ô∏è Estimated time:** 60 minutes\n",
    "\n",
    "**Capstone project combining all learned concepts:**\n",
    "- Data collection and preprocessing\n",
    "- Model training and evaluation\n",
    "- Model deployment and monitoring\n",
    "- Business insights and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete End-to-End ML Pipeline\n",
    "\n",
    "class MLPipeline:\n",
    "    \"\"\"Complete ML pipeline for production use\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.preprocessor = None\n",
    "        self.model = None\n",
    "        self.model_metadata = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def preprocess_data(self, df, target_column=None, test_size=0.2):\n",
    "        \"\"\"Complete data preprocessing pipeline\"\"\"\n",
    "        \n",
    "        print(\"üîß PREPROCESSING DATA\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        # Separate features and target\n",
    "        if target_column:\n",
    "            X = df.drop(columns=[target_column])\n",
    "            y = df[target_column]\n",
    "        else:\n",
    "            X = df\n",
    "            y = None\n",
    "        \n",
    "        # Remove ID columns\n",
    "        id_columns = [col for col in X.columns if 'id' in col.lower()]\n",
    "        X = X.drop(columns=id_columns)\n",
    "        \n",
    "        print(f\"Data shape: {X.shape}\")\n",
    "        print(f\"Features: {list(X.columns)}\")\n",
    "        \n",
    "        # Initialize and fit preprocessor\n",
    "        self.preprocessor = MLPreprocessor(handle_outliers=True)\n",
    "        X_processed = self.preprocessor.fit_transform(X)\n",
    "        \n",
    "        # Split data if target is provided\n",
    "        if y is not None:\n",
    "            is_classification = len(np.unique(y)) < 20\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_processed, y, test_size=test_size, random_state=self.random_state,\n",
    "                stratify=y if is_classification else None\n",
    "            )\n",
    "            \n",
    "            print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "            print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "            \n",
    "            return X_train, X_test, y_train, y_test\n",
    "        else:\n",
    "            return X_processed\n",
    "    \n",
    "    def train_model(self, X_train, y_train, model_type='auto'):\n",
    "        \"\"\"Train the best model for the given problem\"\"\"\n",
    "        \n",
    "        print(\"\\nüéØ TRAINING MODEL\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        # Determine problem type\n",
    "        is_classification = len(np.unique(y_train)) < 20\n",
    "        problem_type = 'classification' if is_classification else 'regression'\n",
    "        \n",
    "        print(f\"Problem type: {problem_type}\")\n",
    "        \n",
    "        # Select models based on problem type\n",
    "        if is_classification:\n",
    "            models = {\n",
    "                'Random Forest': RandomForestClassifier(n_estimators=100, random_state=self.random_state),\n",
    "                'Logistic Regression': LogisticRegression(random_state=self.random_state, max_iter=1000),\n",
    "                'SVM': SVC(probability=True, random_state=self.random_state)\n",
    "            }\n",
    "            scoring = 'accuracy'\n",
    "        else:\n",
    "            models = {\n",
    "                'Random Forest': RandomForestRegressor(n_estimators=100, random_state=self.random_state),\n",
    "                'Linear Regression': LinearRegression(),\n",
    "                'Ridge': Ridge(alpha=1.0, random_state=self.random_state)\n",
    "            }\n",
    "            scoring = 'r2'\n",
    "        \n",
    "        # Compare models using cross-validation\n",
    "        best_score = -np.inf\n",
    "        best_model_name = None\n",
    "        best_model = None\n",
    "        \n",
    "        cv_results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring=scoring)\n",
    "            mean_score = cv_scores.mean()\n",
    "            cv_results[name] = {\n",
    "                'mean_score': mean_score,\n",
    "                'std_score': cv_scores.std(),\n",
    "                'scores': cv_scores\n",
    "            }\n",
    "            \n",
    "            print(f\"{name}: {mean_score:.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_model_name = name\n",
    "                best_model = model\n",
    "        \n",
    "        # Train the best model\n",
    "        print(f\"\\nüèÜ Best model: {best_model_name} (Score: {best_score:.3f})\")\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        self.model = best_model\n",
    "        self.model_metadata = {\n",
    "            'model_name': best_model_name,\n",
    "            'model_type': type(best_model).__name__,\n",
    "            'problem_type': problem_type,\n",
    "            'cv_score': best_score,\n",
    "            'cv_results': cv_results,\n",
    "            'n_features': X_train.shape[1],\n",
    "            'n_samples': X_train.shape[0]\n",
    "        }\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        \n",
    "        print(\"\\nüìä MODEL EVALUATION\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model trained yet. Call train_model() first.\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics based on problem type\n",
    "        is_classification = self.model_metadata['problem_type'] == 'classification'\n",
    "        \n",
    "        if is_classification:\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "            \n",
    "            self.performance_metrics = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1\n",
    "            }\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"Precision: {precision:.3f}\")\n",
    "            print(f\"Recall: {recall:.3f}\")\n",
    "            print(f\"F1-Score: {f1:.3f}\")\n",
    "            \n",
    "        else:\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            self.performance_metrics = {\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'r2_score': r2\n",
    "            }\n",
    "            \n",
    "            print(f\"Mean Squared Error: {mse:.3f}\")\n",
    "            print(f\"Mean Absolute Error: {mae:.3f}\")\n",
    "            print(f\"R¬≤ Score: {r2:.3f}\")\n",
    "        \n",
    "        return self.performance_metrics\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        \n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model trained yet. Call train_model() first.\")\n",
    "        \n",
    "        if self.preprocessor is None:\n",
    "            raise ValueError(\"No preprocessor fitted yet. Call preprocess_data() first.\")\n",
    "        \n",
    "        # Preprocess new data\n",
    "        X_processed = self.preprocessor.transform(X)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(X_processed)\n",
    "        \n",
    "        # Get prediction probabilities for classification\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            probabilities = self.model.predict_proba(X_processed)\n",
    "            return predictions, probabilities\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def generate_insights(self):\n",
    "        \"\"\"Generate business insights from the model\"\"\"\n",
    "        \n",
    "        print(\"\\nüí° BUSINESS INSIGHTS\")\n",
    "        print(\"=\" * 25)\n",
    "        \n",
    "        if self.model is None:\n",
    "            print(\"No model available for insights.\")\n",
    "            return\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': self.preprocessor.feature_names,\n",
    "                'importance': self.model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(\"üîç Top 5 Most Important Features:\")\n",
    "            for i, (_, row) in enumerate(feature_importance.head().iterrows(), 1):\n",
    "                print(f\"  {i}. {row['feature']}: {row['importance']:.3f}\")\n",
    "        \n",
    "        # Model performance summary\n",
    "        print(f\"\\nüìà Model Performance Summary:\")\n",
    "        print(f\"  Model: {self.model_metadata.get('model_name', 'Unknown')}\")\n",
    "        print(f\"  Problem: {self.model_metadata.get('problem_type', 'Unknown')}\")\n",
    "        print(f\"  CV Score: {self.model_metadata.get('cv_score', 0):.3f}\")\n",
    "        \n",
    "        if self.performance_metrics:\n",
    "            for metric, value in self.performance_metrics.items():\n",
    "                print(f\"  {metric.replace('_', ' ').title()}: {value:.3f}\")\n",
    "\n",
    "# Demonstrate complete pipeline\n",
    "print(\"üöÄ COMPLETE ML PIPELINE DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = MLPipeline(random_state=RANDOM_STATE)\n",
    "\n",
    "# Create dataset with target\n",
    "df_pipeline = df_raw.copy()\n",
    "df_pipeline['high_value_customer'] = high_value_target\n",
    "\n",
    "# Run complete pipeline\n",
    "X_train, X_test, y_train, y_test = pipeline.preprocess_data(df_pipeline, 'high_value_customer')\n",
    "best_model = pipeline.train_model(X_train, y_train)\n",
    "performance = pipeline.evaluate_model(X_test, y_test)\n",
    "pipeline.generate_insights()\n",
    "\n",
    "print(\"\\nüéâ PIPELINE COMPLETE!\")\n",
    "print(\"Your ML model is ready for production deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Final Assessment & Next Steps\n",
    "\n",
    "### Congratulations! üéâ\n",
    "\n",
    "You have completed the comprehensive ML & AI 101 training. You now have the skills to:\n",
    "\n",
    "‚úÖ **Build production-ready ML pipelines** with proper data preprocessing and validation  \n",
    "‚úÖ **Implement robust model evaluation** using cross-validation and multiple metrics  \n",
    "‚úÖ **Handle real-world data** from databases, APIs, and web sources  \n",
    "‚úÖ **Deploy and persist models** with proper versioning and metadata  \n",
    "‚úÖ **Generate business insights** from ML models and their predictions  \n",
    "\n",
    "### üéØ Final Project Checklist\n",
    "\n",
    "- [ ] Successfully completed all checkpoint assessments\n",
    "- [ ] Built and evaluated at least 2 different ML models\n",
    "- [ ] Implemented a complete end-to-end pipeline\n",
    "- [ ] Generated actionable business insights\n",
    "- [ ] Saved and loaded models with proper versioning\n",
    "\n",
    "### üöÄ Next Steps in Your ML Journey\n",
    "\n",
    "**Immediate (1-2 weeks):**\n",
    "- Apply these techniques to your own datasets\n",
    "- Explore additional algorithms (XGBoost, LightGBM)\n",
    "- Practice with Kaggle competitions\n",
    "\n",
    "**Short-term (1-3 months):**\n",
    "- Learn deep learning frameworks (TensorFlow, PyTorch)\n",
    "- Explore specialized domains (NLP, Computer Vision, Time Series)\n",
    "- Study MLOps tools (MLflow, Kubeflow, Docker)\n",
    "\n",
    "**Long-term (3-12 months):**\n",
    "- Build end-to-end ML applications\n",
    "- Contribute to open-source ML projects\n",
    "- Stay updated with latest research and techniques\n",
    "\n",
    "### üìö Recommended Resources\n",
    "\n",
    "- **Books:** \"Hands-On Machine Learning\" by Aur√©lien G√©ron\n",
    "- **Courses:** Fast.ai, Coursera ML Specialization\n",
    "- **Practice:** Kaggle, Google Colab, GitHub projects\n",
    "- **Community:** Reddit r/MachineLearning, ML Twitter, local meetups\n",
    "\n",
    "### üíº Career Applications\n",
    "\n",
    "You're now prepared for roles in:\n",
    "- Data Science and Analytics\n",
    "- ML Engineering\n",
    "- Business Intelligence\n",
    "- Product Analytics\n",
    "- Consulting and Strategy\n",
    "\n",
    "**Keep learning, keep building, and keep pushing the boundaries of what's possible with ML!** üåü"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
